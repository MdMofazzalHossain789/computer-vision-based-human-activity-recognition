{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2256095,"sourceType":"datasetVersion","datasetId":1357563}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the required libraries.\nimport os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom collections import deque\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Flatten\nfrom tensorflow.keras.layers import Dropout\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eoAdWDkOJipf","outputId":"5e3437b2-fd99-4412-dc5b-f51e83a2b20e","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:16:56.454691Z","iopub.execute_input":"2025-06-27T09:16:56.454954Z","iopub.status.idle":"2025-06-27T09:17:17.205593Z","shell.execute_reply.started":"2025-06-27T09:16:56.454932Z","shell.execute_reply":"2025-06-27T09:17:17.204710Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"2025-06-27 09:17:00.087396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751015820.512466      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751015820.627511      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# set seeed to get similar values\nseed_constant = 27\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)","metadata":{"id":"nJ9LB9KOKIqd","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.207039Z","iopub.execute_input":"2025-06-27T09:17:17.207690Z","iopub.status.idle":"2025-06-27T09:17:17.213696Z","shell.execute_reply.started":"2025-06-27T09:17:17.207658Z","shell.execute_reply":"2025-06-27T09:17:17.212219Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Specify the directory containing the UCF50 dataset\nDATASET_DIR = \"/kaggle/input/ucf50/UCF50\"\n\nos.path.exists(DATASET_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.214852Z","iopub.execute_input":"2025-06-27T09:17:17.215218Z","iopub.status.idle":"2025-06-27T09:17:17.446944Z","shell.execute_reply.started":"2025-06-27T09:17:17.215191Z","shell.execute_reply":"2025-06-27T09:17:17.446379Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"all_class_names = os.listdir(DATASET_DIR)\n\nlen(all_class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.448709Z","iopub.execute_input":"2025-06-27T09:17:17.448909Z","iopub.status.idle":"2025-06-27T09:17:17.466023Z","shell.execute_reply.started":"2025-06-27T09:17:17.448893Z","shell.execute_reply":"2025-06-27T09:17:17.465395Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Global constant variables -> \nCLASSES_LIST = all_class_names[:4]\n\n# Model Configuration\nIMAGE_HEIGHT, IMAGE_WIDTH = 224, 224\nSEQUENCE_LENGTH = 30\n\n# set drop out rate\nDROPOUT_RATE = 0.5\n\n# set datas\nMAX_VIDEO_PER_CLASS = 30\n\n# split dataset\nTEST_SIZE = 0.25\n\n# model fit parameters\nEPOCHS = 30\nBATCH_SIZE = 4\nVALIDATION_SPLIT = 0.20\n\n\n# give a name of the model to save\nMODEL_NAME = \"Xception\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.466712Z","iopub.execute_input":"2025-06-27T09:17:17.466953Z","iopub.status.idle":"2025-06-27T09:17:17.471327Z","shell.execute_reply.started":"2025-06-27T09:17:17.466932Z","shell.execute_reply":"2025-06-27T09:17:17.470838Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # Create a Matplotlib figure and specify the size of the figure\n# plt.figure(figsize=(20, 20))\n\n# # Get the names of all classes/categories in UCF50\n# all_classes_names = os.listdir(DATASET_DIR)\n\n# # Check if the directory contains classes\n# if not all_classes_names:\n#     raise ValueError(f\"No classes found in {DATASET_DIR}. Please ensure the dataset is extracted correctly.\")\n\n# # Generate a list of 20 random values, ensuring we don't sample more than available classes\n# random_range = random.sample(range(len(all_classes_names)), min(20, len(all_classes_names)))\n\n# # Iterating through all the generated random values\n# for counter, random_index in enumerate(random_range, 1):\n#     selected_class_Name = all_classes_names[random_index]\n#     video_files_names_list = os.listdir(f'{DATASET_DIR}/{selected_class_Name}')\n#     selected_video_file_name = random.choice(video_files_names_list)\n#     video_reader = cv2.VideoCapture(f'{DATASET_DIR}/{selected_class_Name}/{selected_video_file_name}')\n#     _, bgr_frame = video_reader.read()\n#     video_reader.release()\n#     rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n#     cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n#     plt.subplot(5, 4, counter)\n#     plt.imshow(rgb_frame)\n#     plt.axis('off')\n# plt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"dyfcTKP2Og41","outputId":"f5827824-eb16-48b6-a8e5-303852c32c30","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.472388Z","iopub.execute_input":"2025-06-27T09:17:17.472680Z","iopub.status.idle":"2025-06-27T09:17:17.488250Z","shell.execute_reply.started":"2025-06-27T09:17:17.472657Z","shell.execute_reply":"2025-06-27T09:17:17.487752Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# frame extraction\nimport cv2\nimport numpy as np\nimport os\n\ndef frames_extraction(video_path, \n                      sequence_length=SEQUENCE_LENGTH, \n                      image_height=IMAGE_HEIGHT, \n                      image_width=IMAGE_WIDTH):\n    '''\n    This function extracts the required frames from a video, resizes and normalizes them for model input.\n    Args:\n        video_path: The path of the video in the disk, whose frames are to be extracted.\n        sequence_length: Number of frames to extract per video (default: 20).\n        image_height: Height to resize frames to (default: 224 for Xception).\n        image_width: Width to resize frames to (default: 224 for Xception).\n    Returns:\n        frames_list: A list containing the resized and normalized frames of the video, or None if extraction fails.\n    '''\n    # Declare a list to store video frames\n    frames_list = []\n\n    # Check if video file exists\n    if not os.path.exists(video_path):\n        print(f\"Error: Video file not found at {video_path}\")\n        return None\n\n    # Read the video file using VideoCapture\n    video_reader = cv2.VideoCapture(video_path)\n\n    # Check if the video was opened successfully\n    if not video_reader.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        video_reader.release()\n        return None\n\n    # Get the total number of frames in the video\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Ensure the video has enough frames\n    if video_frames_count < sequence_length:\n        print(f\"Warning: Video {video_path} has only {video_frames_count} frames, less than required {sequence_length}\")\n        video_reader.release()\n        return None\n\n    # Calculate the interval after which frames will be sampled\n    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n\n    # Iterate to extract the specified number of frames\n    for frame_counter in range(sequence_length):\n        # Set the current frame position\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n\n        # Read the frame\n        success, frame = video_reader.read()\n\n        # Check if the frame was read successfully\n        if not success or frame is None:\n            print(f\"Warning: Failed to read frame {frame_counter} from {video_path}\")\n            break\n\n        # Resize the frame to the specified dimensions\n        try:\n            resized_frame = cv2.resize(frame, (image_width, image_height))\n        except Exception as e:\n            print(f\"Error resizing frame {frame_counter} from {video_path}: {e}\")\n            break\n\n        # Normalize the frame to [0, 1] for model input\n        normalized_frame = resized_frame / 255.0\n\n        # Append the normalized frame to the list\n        frames_list.append(normalized_frame)\n\n    # Release the VideoCapture object\n    video_reader.release()\n\n    # Ensure the correct number of frames is extracted\n    if len(frames_list) != sequence_length:\n        print(f\"Warning: Extracted {len(frames_list)} frames instead of {sequence_length} from {video_path}\")\n        return None\n\n    # Convert to numpy array for consistency\n    frames_list = np.array(frames_list)\n\n    return frames_list","metadata":{"id":"SYI9_3f4PwHZ","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.488964Z","iopub.execute_input":"2025-06-27T09:17:17.489253Z","iopub.status.idle":"2025-06-27T09:17:17.509243Z","shell.execute_reply.started":"2025-06-27T09:17:17.489237Z","shell.execute_reply":"2025-06-27T09:17:17.508710Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# RUN create dataset function definition\n\nimport os\nimport numpy as np\n\ndef create_dataset(dataset_dir,\n                   classes_list, \n                   sequence_length=SEQUENCE_LENGTH, \n                   image_height=IMAGE_HEIGHT, \n                   image_width=IMAGE_WIDTH, \n                   max_videos_per_class=None):\n    '''\n    This function extracts data for selected classes and creates the dataset.\n    Args:\n        dataset_dir: The directory containing the UCF-50 dataset (e.g., \"/kaggle/working/UCF50_dataset/UCF50\").\n        classes_list: List of class names to include in the dataset.\n        sequence_length: Number of frames to extract per video (default: 20).\n        image_height: Height to resize frames to (default: 224 for Xception).\n        image_width: Width to resize frames to (default: 224 for Xception).\n        max_videos_per_class: Maximum number of videos to process per class (optional, for testing).\n    Returns:\n        features: A numpy array of extracted frame sequences with shape (n_videos, sequence_length, image_height, image_width, 3).\n        labels: A numpy array of class indexes.\n        video_files_paths: A list of video file paths.\n    '''\n    # Initialize lists to store features, labels, and video file paths\n    features = []\n    labels = []\n    video_files_paths = []\n\n    # Check if dataset directory exists\n    if not os.path.exists(dataset_dir):\n        raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n\n    # Iterate through all classes in the classes list\n    for class_index, class_name in enumerate(classes_list):\n        class_path = os.path.join(dataset_dir, class_name)\n        \n        # Check if class directory exists\n        if not os.path.exists(class_path):\n            print(f\"Warning: Class directory not found: {class_path}\")\n            continue\n\n        print(f'Extracting Data of Class: {class_name}')\n\n        # Get the list of video files in the class directory\n        files_list = os.listdir(class_path)\n\n        # Limit the number of videos if specified\n        if max_videos_per_class is not None:\n            files_list = files_list[:max_videos_per_class]\n\n        # Iterate through all video files\n        for file_name in files_list:\n            video_file_path = os.path.join(class_path, file_name)\n\n            # Extract frames using the updated frames_extraction function\n            frames = frames_extraction(video_file_path, sequence_length, image_height, image_width)\n\n            # Skip videos where frame extraction failed\n            if frames is None:\n                print(f\"Skipping video {video_file_path} due to frame extraction failure\")\n                continue\n\n            # Append the data to respective lists\n            features.append(frames)\n            labels.append(class_index)\n            video_files_paths.append(video_file_path)\n\n    # Convert lists to numpy arrays\n    if not features:\n        raise ValueError(\"No valid videos were processed. Check dataset or parameters.\")\n    \n    features = np.asarray(features)\n    labels = np.array(labels)\n\n    print(f\"Dataset created with {len(features)} videos\")\n    print(f\"Features shape: {features.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n\n    return features, labels, video_files_paths","metadata":{"id":"twTJ77AJP9WU","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.509889Z","iopub.execute_input":"2025-06-27T09:17:17.510055Z","iopub.status.idle":"2025-06-27T09:17:17.535495Z","shell.execute_reply.started":"2025-06-27T09:17:17.510041Z","shell.execute_reply":"2025-06-27T09:17:17.535004Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# RUN Create the dataset with explicit parameters\ntry:\n    features, labels, video_files_paths = create_dataset(\n        dataset_dir=DATASET_DIR,\n        classes_list=CLASSES_LIST,\n        sequence_length=SEQUENCE_LENGTH,\n        image_height=IMAGE_HEIGHT,\n        image_width=IMAGE_WIDTH,\n        \n        # Limit to 10 videos per class to manage memory\n        max_videos_per_class=MAX_VIDEO_PER_CLASS  \n    )\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")","metadata":{"id":"VZMH7NYaQGa8","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:17.536096Z","iopub.execute_input":"2025-06-27T09:17:17.536322Z","iopub.status.idle":"2025-06-27T09:17:36.689562Z","shell.execute_reply.started":"2025-06-27T09:17:17.536306Z","shell.execute_reply":"2025-06-27T09:17:36.688827Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Extracting Data of Class: HorseRace\nExtracting Data of Class: VolleyballSpiking\nExtracting Data of Class: Biking\nExtracting Data of Class: TaiChi\nDataset created with 120 videos\nFeatures shape: (120, 30, 224, 224, 3)\nLabels shape: (120,)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# RUN Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"id":"-Y6FgSVkQdWa","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:36.692067Z","iopub.execute_input":"2025-06-27T09:17:36.692285Z","iopub.status.idle":"2025-06-27T09:17:36.695947Z","shell.execute_reply.started":"2025-06-27T09:17:36.692269Z","shell.execute_reply":"2025-06-27T09:17:36.695413Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# RUN Split the Data into Train ( 75% ) and Test Set ( 25% ).\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features,\n                                                                            one_hot_encoded_labels,\n                                                                            test_size = TEST_SIZE,\n                                                                            shuffle = True,\n                                                                            random_state = seed_constant)","metadata":{"id":"foGv-eppQuRQ","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:36.696599Z","iopub.execute_input":"2025-06-27T09:17:36.696826Z","iopub.status.idle":"2025-06-27T09:17:38.190213Z","shell.execute_reply.started":"2025-06-27T09:17:36.696810Z","shell.execute_reply":"2025-06-27T09:17:38.189696Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# RUN create model function definition\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import TimeDistributed, Flatten, LSTM, Dropout, Dense\n\ndef create_xception_lstm_model(sequence_length=SEQUENCE_LENGTH, \n                               image_height=IMAGE_HEIGHT, \n                               image_width=IMAGE_WIDTH, \n                               classes_list=None):\n    '''\n    Constructs the Xception + LSTM model for video classification.\n    Args:\n        sequence_length: Number of frames per video (default: 20).\n        image_height: Height of input frames (default: 224).\n        image_width: Width of input frames (default: 224).\n        classes_list: List of class names for output layer (required).\n    Returns:\n        model: The constructed Xception + LSTM model.\n    '''\n    if classes_list is None:\n        raise ValueError(\"classes_list must be provided to define the output layer size\")\n\n    # Initialize the Sequential model\n    model = Sequential(name=\"Xception_LSTM_Model\")\n\n    try:\n        # Load Xception model with pre-trained ImageNet weights\n        print(\"Loading Xception base model...\")\n        base_model = Xception(\n            weights='imagenet',\n            include_top=False,\n            input_shape=(image_height, image_width, 3)\n        )\n\n        # Freeze the base model to reduce memory usage and training time\n        base_model.trainable = False\n\n        # Create the feature extractor\n        feature_extractor = Model(\n            inputs=base_model.input,\n            outputs=base_model.output,\n            name=\"Xception_Feature_Extractor\"\n        )\n\n        # Add TimeDistributed wrapper to apply Xception to each frame\n        model.add(TimeDistributed(\n            feature_extractor,\n            input_shape=(sequence_length, image_height, image_width, 3),\n            name=\"TimeDistributed_Xception\"\n        ))\n\n        # Flatten the extracted features\n        model.add(TimeDistributed(Flatten(), name=\"TimeDistributed_Flatten\"))\n\n        # Add LSTM layer with fewer units to reduce memory usage\n        model.add(LSTM(64, activation='relu', return_sequences=False, name=\"LSTM_Layer\"))\n\n        # Add Dropout to prevent overfitting\n        model.add(Dropout(DROPOUT_RATE, name=\"Dropout_Layer\"))\n\n        # Output layer for classification\n        model.add(Dense(len(classes_list), activation=\"softmax\", name=\"Output_Layer\"))\n\n        # Print model summary\n        print(\"Model architecture created successfully!\")\n        model.summary()\n\n        return model\n\n    except Exception as e:\n        print(f\"Error creating model: {e}\")\n        return None","metadata":{"id":"0WM_3bdUSE6g","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:38.190875Z","iopub.execute_input":"2025-06-27T09:17:38.191075Z","iopub.status.idle":"2025-06-27T09:17:38.198113Z","shell.execute_reply.started":"2025-06-27T09:17:38.191060Z","shell.execute_reply":"2025-06-27T09:17:38.197479Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# donwload model weights\nfrom tensorflow.keras.applications import Xception\nprint(\"Pre-loading Xception weights...\")\nbase_model = Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\nprint(\"Weights loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:38.199125Z","iopub.execute_input":"2025-06-27T09:17:38.199536Z","iopub.status.idle":"2025-06-27T09:17:44.298907Z","shell.execute_reply.started":"2025-06-27T09:17:38.199518Z","shell.execute_reply":"2025-06-27T09:17:44.298161Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Pre-loading Xception weights...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1751015860.917221      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1751015860.917990      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nWeights loaded successfully!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# RUN\nimport gc\ntf.keras.backend.clear_session()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:44.299739Z","iopub.execute_input":"2025-06-27T09:17:44.299950Z","iopub.status.idle":"2025-06-27T09:17:44.755424Z","shell.execute_reply.started":"2025-06-27T09:17:44.299929Z","shell.execute_reply":"2025-06-27T09:17:44.754713Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Clear previous session to free memory\ntf.keras.backend.clear_session()\n\n# Create the model\nxlstm_model = create_xception_lstm_model(\n    sequence_length=SEQUENCE_LENGTH,\n    image_height=IMAGE_HEIGHT,\n    image_width=IMAGE_WIDTH,\n    classes_list=CLASSES_LIST\n)\n\n# Check if model was created successfully\nif xlstm_model is None:\n    print(\"Failed to create model. Check error messages above.\")\nelse:\n    print(\"Model Created Successfully!\")","metadata":{"id":"nWM-_NprSIJx","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:44.756341Z","iopub.execute_input":"2025-06-27T09:17:44.756595Z","iopub.status.idle":"2025-06-27T09:17:46.318818Z","shell.execute_reply.started":"2025-06-27T09:17:44.756577Z","shell.execute_reply":"2025-06-27T09:17:46.318007Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Loading Xception base model...\nModel architecture created successfully!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"Xception_LSTM_Model\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Xception_LSTM_Model\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ TimeDistributed_Xception             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │      \u001b[38;5;34m20,861,480\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ TimeDistributed_Flatten              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m100352\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ LSTM_Layer (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │      \u001b[38;5;34m25,706,752\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Dropout_Layer (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m260\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ TimeDistributed_Xception             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ TimeDistributed_Flatten              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ LSTM_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,706,752</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Dropout_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,568,492\u001b[0m (177.64 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,568,492</span> (177.64 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,707,012\u001b[0m (98.06 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,707,012</span> (98.06 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Model Created Successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Plot the structure of the contructed model.\nplot_model(xlstm_model, to_file = f'{MODEL_NAME}_model_Plot.png', show_shapes = True, show_layer_names = True)\n\nprint(f\"{MODEL_NAME} Model Plot saved successfully...\")","metadata":{"id":"ZViNPAJISIr_","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:46.319792Z","iopub.execute_input":"2025-06-27T09:17:46.320054Z","iopub.status.idle":"2025-06-27T09:17:46.648724Z","shell.execute_reply.started":"2025-06-27T09:17:46.320031Z","shell.execute_reply":"2025-06-27T09:17:46.647856Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Xception Model Plot saved successfully...\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Create an Instance of Early Stopping Callback\nearly_stopping_callback = EarlyStopping(monitor = 'val_loss', \n                                        patience = 10, \n                                        mode = 'min', \n                                        restore_best_weights = True)\n\n# Compile the model and specify loss function, optimizer and metrics values to the model\nxlstm_model.compile(loss = 'categorical_crossentropy', \n                    optimizer = 'Adam', \n                    metrics = [\"accuracy\"])\n\n# Start training the model.\nconvlstm_model_training_history = xlstm_model.fit(x = features_train,\n                                                     y = labels_train,\n                                                     epochs = EPOCHS,\n                                                     batch_size = BATCH_SIZE,\n                                                     shuffle = True,\n                                                     validation_split = VALIDATION_SPLIT,\n                                                     callbacks = [early_stopping_callback])","metadata":{"id":"dNZIILH5Savo","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:17:46.649905Z","iopub.execute_input":"2025-06-27T09:17:46.650222Z","iopub.status.idle":"2025-06-27T09:30:42.561233Z","shell.execute_reply.started":"2025-06-27T09:17:46.650194Z","shell.execute_reply":"2025-06-27T09:30:42.560491Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1751016005.538112     580 service.cc:148] XLA service 0x7dee4003bf30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1751016005.540011     580 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1751016005.540042     580 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1751016027.956195     580 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1751016047.099829     580 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 6s/step - accuracy: 0.2137 - loss: 2982.4739 - val_accuracy: 0.5000 - val_loss: 3980.3840\nEpoch 2/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2622 - loss: 3354.4602 - val_accuracy: 0.0556 - val_loss: 3672.1533\nEpoch 3/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3133 - loss: 3071.5605 - val_accuracy: 0.3889 - val_loss: 1792.9009\nEpoch 4/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2589 - loss: 2290.5063 - val_accuracy: 0.5000 - val_loss: 3514.5850\nEpoch 5/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2277 - loss: 4787.5806 - val_accuracy: 0.2222 - val_loss: 5395.0454\nEpoch 6/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.1444 - loss: 6150.1694 - val_accuracy: 0.1667 - val_loss: 3517.7986\nEpoch 7/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2198 - loss: 3357.8694 - val_accuracy: 0.1667 - val_loss: 1859.3275\nEpoch 8/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2438 - loss: 2795.5520 - val_accuracy: 0.1667 - val_loss: 1493.3158\nEpoch 9/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2758 - loss: 1855.8098 - val_accuracy: 0.1667 - val_loss: 1070.7130\nEpoch 10/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2927 - loss: 1672.2474 - val_accuracy: 0.3333 - val_loss: 566.7767\nEpoch 11/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2401 - loss: 2309.1536 - val_accuracy: 0.2778 - val_loss: 697.9626\nEpoch 12/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2957 - loss: 812.5129 - val_accuracy: 0.3889 - val_loss: 879.6865\nEpoch 13/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2921 - loss: 1953.0398 - val_accuracy: 0.5556 - val_loss: 471.7047\nEpoch 14/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2708 - loss: 3212.7771 - val_accuracy: 0.3889 - val_loss: 690.8135\nEpoch 15/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3952 - loss: 2053.2966 - val_accuracy: 0.3333 - val_loss: 305.6983\nEpoch 16/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3017 - loss: 1965.0958 - val_accuracy: 0.3333 - val_loss: 657.8129\nEpoch 17/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1502 - loss: 3217.9780 - val_accuracy: 0.3889 - val_loss: 729.1494\nEpoch 18/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1462 - loss: 3790.4167 - val_accuracy: 0.3333 - val_loss: 1981.9165\nEpoch 19/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2069 - loss: 4032.8000 - val_accuracy: 0.3333 - val_loss: 1541.6130\nEpoch 20/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2698 - loss: 3648.0725 - val_accuracy: 0.1667 - val_loss: 1009.2016\nEpoch 21/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1866 - loss: 2058.7693 - val_accuracy: 0.3333 - val_loss: 775.9207\nEpoch 22/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2723 - loss: 726.6761 - val_accuracy: 0.2778 - val_loss: 2768.9565\nEpoch 23/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1673 - loss: 1562.3750 - val_accuracy: 0.2778 - val_loss: 335.9544\nEpoch 24/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1981 - loss: 1525.2509 - val_accuracy: 0.1667 - val_loss: 2048.3916\nEpoch 25/30\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2054 - loss: 1809.9901 - val_accuracy: 0.2222 - val_loss: 1459.6212\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# previous code\n# model_evaluation_history = xlstm_model.evaluate(features_test, labels_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the trained model.\n\n# Create a dataset from your test features and labels\ntest_dataset = tf.data.Dataset.from_tensor_slices((features_test, labels_test))\ntest_dataset = test_dataset.batch(8)  # Try 4, 8, 16 depending on your memory\n\n# Evaluate using the dataset\nmodel_evaluation_history = xlstm_model.evaluate(test_dataset)","metadata":{"id":"9qgXivorTN6F","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:38:31.755790Z","iopub.execute_input":"2025-06-27T09:38:31.756379Z","iopub.status.idle":"2025-06-27T09:40:07.507288Z","shell.execute_reply.started":"2025-06-27T09:38:31.756349Z","shell.execute_reply":"2025-06-27T09:40:07.506758Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 9s/step - accuracy: 0.2442 - loss: 709.9626\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# # Get the loss and accuracy from model_evaluation_history.\n# model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n\n# # Define the string date format.\n# # Get the current Date and Time in a DateTime Object.\n# # Convert the DateTime object to string according to the style mentioned in date_time_format string.\n# date_time_format = '%Y_%m_%d__%H_%M_%S'\n# current_date_time_dt = dt.datetime.now()\n# current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n\n# # Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n# model_file_name = f'xlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n\n# # Save your Model.\n# xlstm_model.save(model_file_name)","metadata":{"id":"zA_1ExNmTWPl","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.293354Z","iopub.status.idle":"2025-06-27T09:31:23.293690Z","shell.execute_reply.started":"2025-06-27T09:31:23.293533Z","shell.execute_reply":"2025-06-27T09:31:23.293548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n    '''\n    This function will plot the metrics passed to it in a graph.\n    Args:\n        model_training_history: A history object containing a record of training and validation\n                                loss values and metrics values at successive epochs\n        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n        plot_name:              The title of the graph.\n    '''\n\n    # Get metric values using metric names as identifiers.\n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n\n    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n    epochs = range(len(metric_value_1))\n\n    # Plot the Graph.\n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n\n    # Add title to the plot.\n    plt.title(str(plot_name))\n\n    # Add legend to the plot.\n    plt.legend()","metadata":{"id":"hZiTEY5fUags","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.295401Z","iopub.status.idle":"2025-06-27T09:31:23.295730Z","shell.execute_reply.started":"2025-06-27T09:31:23.295581Z","shell.execute_reply":"2025-06-27T09:31:23.295595Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the training and validation loss metrices.\nplot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')","metadata":{"id":"-8mabamsUexe","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.296741Z","iopub.status.idle":"2025-06-27T09:31:23.297030Z","shell.execute_reply.started":"2025-06-27T09:31:23.296888Z","shell.execute_reply":"2025-06-27T09:31:23.296901Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the training and validation accuracy metrices.\nplot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')","metadata":{"id":"uhJ1tYlUUoNF","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.298043Z","iopub.status.idle":"2025-06-27T09:31:23.298338Z","shell.execute_reply.started":"2025-06-27T09:31:23.298183Z","shell.execute_reply":"2025-06-27T09:31:23.298195Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def download_youtube_videos(youtube_video_url, output_directory):\n     '''\n    This function downloads the youtube video whose URL is passed to it as an argument.\n    Args:\n        youtube_video_url: URL of the video that is required to be downloaded.\n        output_directory:  The directory path to which the video needs to be stored after downloading.\n    Returns:\n        title: The title of the downloaded youtube video.\n    '''\n\n     # Create a video object which contains useful information about the video.\n     video = pafy.new(youtube_video_url)\n\n     # Retrieve the title of the video.\n     title = video.title\n\n     # Get the best available quality object for the video.\n     video_best = video.getbest()\n\n     # Construct the output file path.\n     output_file_path = f'{output_directory}/{title}.mp4'\n\n     # Download the youtube video at the best available quality and store it to the contructed path.\n     video_best.download(filepath = output_file_path, quiet = True)\n\n     # Return the video title.\n     return title","metadata":{"id":"hZou9bN2U0hY","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.299292Z","iopub.status.idle":"2025-06-27T09:31:23.299611Z","shell.execute_reply.started":"2025-06-27T09:31:23.299461Z","shell.execute_reply":"2025-06-27T09:31:23.299474Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make the Output directory if it does not exist\ntest_videos_directory = 'test_videos'\nos.makedirs(test_videos_directory, exist_ok = True)\n\n# Download a YouTube Video.\nvideo_title = download_youtube_videos('https://www.youtube.com/watch?v=8u0qjmHIOcE', test_videos_directory)\n\n# Get the YouTube Video's path we just downloaded.\ninput_video_file_path = f'{test_videos_directory}/{video_title}.mp4'","metadata":{"id":"s9paRRgzVOPp","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.300488Z","iopub.status.idle":"2025-06-27T09:31:23.300761Z","shell.execute_reply.started":"2025-06-27T09:31:23.300622Z","shell.execute_reply":"2025-06-27T09:31:23.300634Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n    '''\n    This function will perform action recognition on a video using the LRCN model.\n    Args:\n    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n    '''\n\n    # Initialize the VideoCapture object to read from the video file.\n    video_reader = cv2.VideoCapture(video_file_path)\n\n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Initialize the VideoWriter Object to store the output video in the disk.\n    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'),\n                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n\n    # Declare a queue to store video frames.\n    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n\n    # Initialize a variable to store the predicted action being performed in the video.\n    predicted_class_name = ''\n\n    # Iterate until the video is accessed successfully.\n    while video_reader.isOpened():\n\n        # Read the frame.\n        ok, frame = video_reader.read()\n\n        # Check if frame is not read properly then break the loop.\n        if not ok:\n            break\n\n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n\n        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n        normalized_frame = resized_frame / 255\n\n        # Appending the pre-processed frame into the frames list.\n        frames_queue.append(normalized_frame)\n\n        # Check if the number of frames in the queue are equal to the fixed sequence length.\n        if len(frames_queue) == SEQUENCE_LENGTH:\n\n            # Pass the normalized frames to the model and get the predicted probabilities.\n            predicted_labels_probabilities = convlstm_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n\n            # Get the index of class with highest probability.\n            predicted_label = np.argmax(predicted_labels_probabilities)\n\n            # Get the class name using the retrieved index.\n            predicted_class_name = CLASSES_LIST[predicted_label]\n\n        # Write predicted class name on top of the frame.\n        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n        # Write The frame into the disk using the VideoWriter Object.\n        video_writer.write(frame)\n\n    # Release the VideoCapture and VideoWriter objects.\n    video_reader.release()\n    video_writer.release()","metadata":{"id":"G7BtQli7WGrv","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.301524Z","iopub.status.idle":"2025-06-27T09:31:23.301806Z","shell.execute_reply.started":"2025-06-27T09:31:23.301668Z","shell.execute_reply":"2025-06-27T09:31:23.301680Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Construct the output video path.\noutput_video_file_path = f'new/Output-SeqLen.mp4'\n\n# Perform Action Recognition on the Test Video.\npredict_on_video(\"/content/Test Video.mp4\", output_video_file_path, SEQUENCE_LENGTH)\n\n# Display the output video.\nVideoFileClip(output_video_file_path, audio=False, target_resolution=(300,None)).ipython_display()","metadata":{"id":"TPH4-PfsWLz2","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.302725Z","iopub.status.idle":"2025-06-27T09:31:23.302956Z","shell.execute_reply.started":"2025-06-27T09:31:23.302852Z","shell.execute_reply":"2025-06-27T09:31:23.302863Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom collections import deque\nfrom moviepy.editor import VideoFileClip\n\n# Define model-related constants\nSEQUENCE_LENGTH = 20  # Set this based on your model\nIMAGE_HEIGHT, IMAGE_WIDTH = 64, 64  # Set this based on your training\nCLASSES_LIST = ['Walking', 'Running', 'Jumping']  # Replace with your actual class names\n\n# Load your trained model (assumes it's already loaded globally as convlstm_model)\n# convlstm_model = ... (your model loading code here)\n\ndef predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n    '''\n    Perform action recognition on a video using the LRCN model.\n\n    Args:\n    - video_file_path: str, input video path\n    - output_file_path: str, where to save output video with predictions\n    - SEQUENCE_LENGTH: int, number of frames per prediction sequence\n    '''\n\n    # Initialize the VideoCapture object\n    video_reader = cv2.VideoCapture(video_file_path)\n\n    # Get video properties\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = video_reader.get(cv2.CAP_PROP_FPS)\n\n    # Create VideoWriter object\n    video_writer = cv2.VideoWriter(\n        output_file_path,\n        cv2.VideoWriter_fourcc(*'mp4v'),\n        fps,\n        (original_video_width, original_video_height)\n    )\n\n    # Frame queue and predicted class label\n    frames_queue = deque(maxlen=SEQUENCE_LENGTH)\n    predicted_class_name = ''\n\n    while True:\n        ok, frame = video_reader.read()\n\n        # Break loop if no frame is returned\n        if not ok or frame is None:\n            break\n\n        # Resize and normalize frame\n        resized_frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n        normalized_frame = resized_frame / 255.0\n\n        # Append to queue\n        frames_queue.append(normalized_frame)\n\n        # Predict action when enough frames are collected\n        if len(frames_queue) == SEQUENCE_LENGTH:\n            input_batch = np.expand_dims(frames_queue, axis=0)  # Shape: (1, SEQUENCE_LENGTH, H, W, 3)\n            predicted_probabilities = convlstm_model.predict(input_batch)[0]\n            predicted_label = np.argmax(predicted_probabilities)\n            predicted_class_name = CLASSES_LIST[predicted_label]\n\n        # Overlay prediction on original frame\n        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n                    1, (0, 255, 0), 2, cv2.LINE_AA)\n\n        # Write frame to output\n        video_writer.write(frame)\n\n    # Release resources\n    video_reader.release()\n    video_writer.release()\n    print(f\"Prediction complete. Output saved to {output_file_path}\")\n\n# ==== Run the prediction ====\n\n# Example usage\ninput_video_path = \"/content/Test Video.mp4\"\noutput_video_path = \"new/Output-SeqLen.mp4\"\n\n# Make sure output folder exists\nimport os\nos.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n\n# Run prediction\npredict_on_video(input_video_path, output_video_path, SEQUENCE_LENGTH)\n\n# Display the output video\nVideoFileClip(output_video_path, audio=False, target_resolution=(300, None)).ipython_display()\n","metadata":{"id":"6bxcQQ6rXmN6","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.303915Z","iopub.status.idle":"2025-06-27T09:31:23.304135Z","shell.execute_reply.started":"2025-06-27T09:31:23.304038Z","shell.execute_reply":"2025-06-27T09:31:23.304047Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://drive.google.com/file/d/1c7j6_zgCsC1wq1Zxg-gZE_6Zk_Fo-UZj/view?usp=sharing","metadata":{"id":"_0_g5lHxYmPE","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.305004Z","iopub.status.idle":"2025-06-27T09:31:23.305681Z","shell.execute_reply.started":"2025-06-27T09:31:23.305548Z","shell.execute_reply":"2025-06-27T09:31:23.305567Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"6J2YsZG7Y0B_","trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:31:23.306348Z","iopub.status.idle":"2025-06-27T09:31:23.306625Z","shell.execute_reply.started":"2025-06-27T09:31:23.306515Z","shell.execute_reply":"2025-06-27T09:31:23.306529Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}