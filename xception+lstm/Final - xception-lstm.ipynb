{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2256095,"sourceType":"datasetVersion","datasetId":1357563},{"sourceId":12316740,"sourceType":"datasetVersion","datasetId":7763484}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import the required libraries.\nimport os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom collections import deque\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Flatten\nfrom tensorflow.keras.layers import Dropout\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"eoAdWDkOJipf","outputId":"5e3437b2-fd99-4412-dc5b-f51e83a2b20e","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:15.120007Z","iopub.execute_input":"2025-06-29T10:20:15.120685Z","iopub.status.idle":"2025-06-29T10:20:31.037247Z","shell.execute_reply.started":"2025-06-29T10:20:15.120657Z","shell.execute_reply":"2025-06-29T10:20:31.036395Z"}},"outputs":[{"name":"stderr","text":"2025-06-29 10:20:17.603414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751192417.857851      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751192417.942105      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# set seeed to get similar values\nseed_constant = 27\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)","metadata":{"id":"nJ9LB9KOKIqd","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.038606Z","iopub.execute_input":"2025-06-29T10:20:31.039072Z","iopub.status.idle":"2025-06-29T10:20:31.043133Z","shell.execute_reply.started":"2025-06-29T10:20:31.039053Z","shell.execute_reply":"2025-06-29T10:20:31.042513Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Specify the directory containing the UCF50 dataset\nDATASET_DIR = \"/kaggle/input/ucf50/UCF50\"\n\nos.path.exists(DATASET_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.044010Z","iopub.execute_input":"2025-06-29T10:20:31.044273Z","iopub.status.idle":"2025-06-29T10:20:31.072921Z","shell.execute_reply.started":"2025-06-29T10:20:31.044250Z","shell.execute_reply":"2025-06-29T10:20:31.072314Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"all_class_names = os.listdir(DATASET_DIR)\n\nlen(all_class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.074310Z","iopub.execute_input":"2025-06-29T10:20:31.074556Z","iopub.status.idle":"2025-06-29T10:20:31.105038Z","shell.execute_reply.started":"2025-06-29T10:20:31.074534Z","shell.execute_reply":"2025-06-29T10:20:31.104507Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Global constant variables -> \nNO_OF_CLASSES = 6\nCLASSES_LIST = all_class_names[:NO_OF_CLASSES]\n\n# Model Configuration\nIMAGE_HEIGHT, IMAGE_WIDTH = 224, 224\nSEQUENCE_LENGTH = 15\n\n# set drop out rate\nDROPOUT_RATE = 0.2\n\n# set datas\nMAX_VIDEO_PER_CLASS = 100\n\n# split dataset\nTEST_SIZE = 0.20\n\n# model fit parameters\nEPOCHS = 50\nBATCH_SIZE = 6\nVALIDATION_SPLIT = 0.20\n\n\n# give a name of the model to save\nMODEL_NAME = \"Xception\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.105660Z","iopub.execute_input":"2025-06-29T10:20:31.105831Z","iopub.status.idle":"2025-06-29T10:20:31.110175Z","shell.execute_reply.started":"2025-06-29T10:20:31.105817Z","shell.execute_reply":"2025-06-29T10:20:31.109515Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # Create a Matplotlib figure and specify the size of the figure\n# plt.figure(figsize=(20, 20))\n\n# # Get the names of all classes/categories in UCF50\n# all_classes_names = os.listdir(DATASET_DIR)\n\n# # Check if the directory contains classes\n# if not all_classes_names:\n#     raise ValueError(f\"No classes found in {DATASET_DIR}. Please ensure the dataset is extracted correctly.\")\n\n# # Generate a list of 20 random values, ensuring we don't sample more than available classes\n# random_range = random.sample(range(len(all_classes_names)), min(20, len(all_classes_names)))\n\n# # Iterating through all the generated random values\n# for counter, random_index in enumerate(random_range, 1):\n#     selected_class_Name = all_classes_names[random_index]\n#     video_files_names_list = os.listdir(f'{DATASET_DIR}/{selected_class_Name}')\n#     selected_video_file_name = random.choice(video_files_names_list)\n#     video_reader = cv2.VideoCapture(f'{DATASET_DIR}/{selected_class_Name}/{selected_video_file_name}')\n#     _, bgr_frame = video_reader.read()\n#     video_reader.release()\n#     rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n#     cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n#     plt.subplot(5, 4, counter)\n#     plt.imshow(rgb_frame)\n#     plt.axis('off')\n# plt.show()","metadata":{"id":"dyfcTKP2Og41","outputId":"f5827824-eb16-48b6-a8e5-303852c32c30","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.110905Z","iopub.execute_input":"2025-06-29T10:20:31.111160Z","iopub.status.idle":"2025-06-29T10:20:31.128805Z","shell.execute_reply.started":"2025-06-29T10:20:31.111143Z","shell.execute_reply":"2025-06-29T10:20:31.128142Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# frame extraction\nimport cv2\nimport numpy as np\nimport os\n\ndef frames_extraction(video_path, \n                      sequence_length=SEQUENCE_LENGTH, \n                      image_height=IMAGE_HEIGHT, \n                      image_width=IMAGE_WIDTH):\n    # Declare a list to store video frames\n    frames_list = []\n\n    # Check if video file exists\n    if not os.path.exists(video_path):\n        print(f\"Error: Video file not found at {video_path}\")\n        return None\n\n    # Read the video file using VideoCapture\n    video_reader = cv2.VideoCapture(video_path)\n\n    # Check if the video was opened successfully\n    if not video_reader.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        video_reader.release()\n        return None\n\n    # Get the total number of frames in the video\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Ensure the video has enough frames\n    if video_frames_count < sequence_length:\n        print(f\"Warning: Video {video_path} has only {video_frames_count} frames, less than required {sequence_length}\")\n        video_reader.release()\n        return None\n\n    # Calculate the interval after which frames will be sampled\n    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n\n    # Iterate to extract the specified number of frames\n    for frame_counter in range(sequence_length):\n        # Set the current frame position\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n\n        # Read the frame\n        success, frame = video_reader.read()\n\n        # Check if the frame was read successfully\n        if not success or frame is None:\n            print(f\"Warning: Failed to read frame {frame_counter} from {video_path}\")\n            break\n\n        # Resize the frame to the specified dimensions\n        try:\n            resized_frame = cv2.resize(frame, (image_width, image_height))\n        except Exception as e:\n            print(f\"Error resizing frame {frame_counter} from {video_path}: {e}\")\n            break\n\n        # Normalize the frame to [0, 1] for model input\n        normalized_frame = resized_frame / 255.0\n\n        # Append the normalized frame to the list\n        frames_list.append(normalized_frame)\n\n    # Release the VideoCapture object\n    video_reader.release()\n\n    # Ensure the correct number of frames is extracted\n    if len(frames_list) != sequence_length:\n        print(f\"Warning: Extracted {len(frames_list)} frames instead of {sequence_length} from {video_path}\")\n        return None\n\n    # Convert to numpy array for consistency\n    frames_list = np.array(frames_list)\n\n    return frames_list","metadata":{"id":"SYI9_3f4PwHZ","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.129585Z","iopub.execute_input":"2025-06-29T10:20:31.129817Z","iopub.status.idle":"2025-06-29T10:20:31.152782Z","shell.execute_reply.started":"2025-06-29T10:20:31.129793Z","shell.execute_reply":"2025-06-29T10:20:31.152158Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# RUN create dataset function definition\n\nimport os\nimport numpy as np\n\ndef create_dataset(dataset_dir,\n                   classes_list, \n                   sequence_length=SEQUENCE_LENGTH, \n                   image_height=IMAGE_HEIGHT, \n                   image_width=IMAGE_WIDTH, \n                   max_videos_per_class=None):\n    '''\n    This function extracts data for selected classes and creates the dataset.\n    Args:\n        dataset_dir: The directory containing the UCF-50 dataset (e.g., \"/kaggle/working/UCF50_dataset/UCF50\").\n        classes_list: List of class names to include in the dataset.\n        sequence_length: Number of frames to extract per video (default: 20).\n        image_height: Height to resize frames to (default: 224 for Xception).\n        image_width: Width to resize frames to (default: 224 for Xception).\n        max_videos_per_class: Maximum number of videos to process per class (optional, for testing).\n    Returns:\n        features: A numpy array of extracted frame sequences with shape (n_videos, sequence_length, image_height, image_width, 3).\n        labels: A numpy array of class indexes.\n        video_files_paths: A list of video file paths.\n    '''\n    # Initialize lists to store features, labels, and video file paths\n    features = []\n    labels = []\n\n    # Check if dataset directory exists\n    if not os.path.exists(dataset_dir):\n        raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n\n    # Iterate through all classes in the classes list\n    for class_index, class_name in enumerate(classes_list):\n        class_path = os.path.join(dataset_dir, class_name)\n        \n        # Check if class directory exists\n        if not os.path.exists(class_path):\n            print(f\"Warning: Class directory not found: {class_path}\")\n            continue\n\n        print(f'Extracting Data of Class: {class_name}')\n\n        # Get the list of video files in the class directory\n        files_list = os.listdir(class_path)\n\n        # Limit the number of videos if specified\n        if max_videos_per_class is not None:\n            files_list = files_list[:max_videos_per_class]\n\n        # Iterate through all video files\n        for file_name in files_list:\n            video_file_path = os.path.join(class_path, file_name)\n\n            # Extract frames using the updated frames_extraction function\n            frames = frames_extraction(video_file_path, sequence_length, image_height, image_width)\n\n            # Skip videos where frame extraction failed\n            if frames is None:\n                print(f\"Skipping video {video_file_path} due to frame extraction failure\")\n                continue\n\n            # Append the data to respective lists\n            features.append(frames)\n            labels.append(class_index)\n\n    # Convert lists to numpy arrays\n    if not features:\n        raise ValueError(\"No valid videos were processed. Check dataset or parameters.\")\n    \n    features = np.asarray(features)\n    labels = np.array(labels)\n\n    print(f\"Dataset created with {len(features)} videos\")\n    print(f\"Features shape: {features.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n\n    return features, labels","metadata":{"id":"twTJ77AJP9WU","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.153463Z","iopub.execute_input":"2025-06-29T10:20:31.153756Z","iopub.status.idle":"2025-06-29T10:20:31.173011Z","shell.execute_reply.started":"2025-06-29T10:20:31.153729Z","shell.execute_reply":"2025-06-29T10:20:31.172403Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# RUN Create the dataset with explicit parameters\ntry:\n    features, labels = create_dataset(\n        dataset_dir=DATASET_DIR,\n        classes_list=CLASSES_LIST,\n        sequence_length=SEQUENCE_LENGTH,\n        image_height=IMAGE_HEIGHT,\n        image_width=IMAGE_WIDTH,\n        \n        # Limit to 10 videos per class to manage memory\n        max_videos_per_class=MAX_VIDEO_PER_CLASS  \n    )\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")","metadata":{"id":"VZMH7NYaQGa8","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:20:31.173841Z","iopub.execute_input":"2025-06-29T10:20:31.174077Z","iopub.status.idle":"2025-06-29T10:21:25.897973Z","shell.execute_reply.started":"2025-06-29T10:20:31.174054Z","shell.execute_reply":"2025-06-29T10:21:25.897338Z"}},"outputs":[{"name":"stdout","text":"Extracting Data of Class: HorseRace\nExtracting Data of Class: VolleyballSpiking\nExtracting Data of Class: Biking\nExtracting Data of Class: TaiChi\nExtracting Data of Class: Punch\nExtracting Data of Class: BreastStroke\nDataset created with 600 videos\nFeatures shape: (600, 15, 224, 224, 3)\nLabels shape: (600,)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n# Define augmentation functions\ndef horizontal_flip(frames):\n    # Flip each frame in the sequence\n    print(\"flipping...\")\n    return [cv2.flip(frame, 1) for frame in frames]\n\ndef random_shear_frames(frames):\n    print(\"shearing...\")\n    # Shear each frame in the sequence\n    return [random_shear(frame) for frame in frames]\n\ndef random_shear(frame):\n    shear_x = random.uniform(-0.09, 0.09)\n    shear_y = random.uniform(0.1, 0.3)\n    height, width = frame.shape[:2]\n    M = np.float32([[1, shear_x, 0], [shear_y, 1, 0]])\n    return cv2.warpAffine(frame, M, (width, height))\n\ndef salt_and_pepper_noise_frames(frames):\n    print(\"adding noise...\")\n    # Add salt and pepper noise to each frame in the sequence\n    return [salt_and_pepper_noise(frame) for frame in frames]\n\ndef salt_and_pepper_noise(frame, amount=0.02):\n    output = np.copy(frame)\n    total_pixels = frame.size\n    num_salt = int(total_pixels * amount)\n    num_pepper = int(total_pixels * amount)\n    \n    for _ in range(num_salt):\n        i = random.randint(0, frame.shape[0] - 1)\n        j = random.randint(0, frame.shape[1] - 1)\n        output[i, j] = 255  # Salt (white pixel)\n    \n    for _ in range(num_pepper):\n        i = random.randint(0, frame.shape[0] - 1)\n        j = random.randint(0, frame.shape[1] - 1)\n        output[i, j] = 0  # Pepper (black pixel)\n    \n    return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:25.900158Z","iopub.execute_input":"2025-06-29T10:21:25.900378Z","iopub.status.idle":"2025-06-29T10:21:25.907366Z","shell.execute_reply.started":"2025-06-29T10:21:25.900360Z","shell.execute_reply":"2025-06-29T10:21:25.906788Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"len(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:25.907989Z","iopub.execute_input":"2025-06-29T10:21:25.908168Z","iopub.status.idle":"2025-06-29T10:21:25.935920Z","shell.execute_reply.started":"2025-06-29T10:21:25.908154Z","shell.execute_reply":"2025-06-29T10:21:25.935258Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"600"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence\n\n# Define your data generator\nclass VideoDataGenerator(Sequence):\n    def __init__(self, video_frames, labels, batch_size, augmentations=None):\n        self.video_frames = video_frames  # List of sequences, where each sequence is a list of frames\n        self.labels = labels\n        self.batch_size = batch_size\n        self.augmentations = augmentations or []  # List of augmentation functions\n    \n    def __len__(self):\n        # Return the number of batches per epoch\n        return int(np.ceil(len(self.video_frames) / self.batch_size))\n    \n    def __getitem__(self, idx):\n        # Get batch of video sequences and corresponding labels\n        batch_video_frames = self.video_frames[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        # Apply augmentations to each sequence\n        augmented_frames = [self.apply_augmentations(sequence) for sequence in batch_video_frames]\n        \n        return np.array(augmented_frames), np.array(batch_labels)\n    \n    def apply_augmentations(self, sequence):\n        # Apply augmentations to each frame in the sequence\n        augmented_sequence = sequence\n        for aug in self.augmentations:\n            augmented_sequence = aug(augmented_sequence)  # Apply augmentation to the entire sequence\n        return augmented_sequence\n    \n    def on_epoch_end(self):\n        # Shuffle at the end of each epoch if needed\n        pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:25.936623Z","iopub.execute_input":"2025-06-29T10:21:25.936835Z","iopub.status.idle":"2025-06-29T10:21:25.952849Z","shell.execute_reply.started":"2025-06-29T10:21:25.936811Z","shell.execute_reply":"2025-06-29T10:21:25.952145Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# combined = zip(labels, features)\n\n# augmented_features = list(features) # 60 -> 61\n# augmented_labels = list(labels)\n\n# # video frame = [frame 1, 2 3 4 ]\n# print(\"Started augmentation frames...\")\n\n# for label, video_frames in combined:\n    \n#     # flip\n#     augmented_features.append(horizontal_flip(video_frames))\n#     augmented_labels.append(label)\n    \n#     # shear\n#     augmented_features.append(random_shear_frames(video_frames))\n#     augmented_labels.append(label)\n    \n#     # noise\n#     augmented_features.append(salt_and_pepper_noise_frames(video_frames))\n#     augmented_labels.append(label)\n    \n#     # augment all together\n#     # augmented_features.append(augment_all_together(video_frames))\n#     # augmented_labels.append(label)\n    \n\n#     #del flipped_video\n\n# print(\"Data augmented successfully.\")\n# augmented_features = np.asarray(augmented_features)\n# augmented_labels = np.asarray(augmented_labels)\n\n# print(f\"Old videos {len(features)}, new videos {len(augmented_features)}\")\n# del features, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:25.953676Z","iopub.execute_input":"2025-06-29T10:21:25.954407Z","iopub.status.idle":"2025-06-29T10:21:25.970788Z","shell.execute_reply.started":"2025-06-29T10:21:25.954390Z","shell.execute_reply":"2025-06-29T10:21:25.970067Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# input(\"Press any key to continue...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:25.971551Z","iopub.execute_input":"2025-06-29T10:21:25.971782Z","iopub.status.idle":"2025-06-29T10:21:25.988198Z","shell.execute_reply.started":"2025-06-29T10:21:25.971767Z","shell.execute_reply":"2025-06-29T10:21:25.987560Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def display_keyframes(keyframes):\n    for idx, keyframe in enumerate(keyframes):\n        # Ensure the keyframe is in the right data type and scale\n        if keyframe.dtype != np.uint8:\n            keyframe = np.clip(keyframe, 0, 1)  # If normalized\n            keyframe = (keyframe * 255).astype(np.uint8)\n\n        plt.figure(figsize=(5, 5))\n        plt.imshow(cv2.cvtColor(keyframe, cv2.COLOR_BGR2RGB))\n        plt.title(f'Keyframe {idx + 1}')\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:25.988987Z","iopub.execute_input":"2025-06-29T10:21:25.989341Z","iopub.status.idle":"2025-06-29T10:21:26.002957Z","shell.execute_reply.started":"2025-06-29T10:21:25.989313Z","shell.execute_reply":"2025-06-29T10:21:26.002200Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def display_keyframes_side_by_side(keyframes1, keyframes2, max_pairs=None):\n    # Use the minimum number of keyframes to avoid index errors\n    num_keyframes = min(len(keyframes1), len(keyframes2))\n    if max_pairs is not None:\n        num_keyframes = min(num_keyframes, max_pairs)\n\n    for idx in range(num_keyframes):\n        kf1 = keyframes1[idx]\n        kf2 = keyframes2[idx]\n\n        # Convert both keyframes to uint8 and BGR to RGB\n        def prepare(frame):\n            if frame.dtype != np.uint8:\n                frame = np.clip(frame, 0, 1)\n                frame = (frame * 255).astype(np.uint8)\n            return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        kf1 = prepare(kf1)\n        kf2 = prepare(kf2)\n\n        # Show side-by-side\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.imshow(kf1)\n        plt.title(f'Original Frame - Keyframe {idx + 1}')\n        plt.axis('off')\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(kf2)\n        plt.title(f'Augmented - Keyframe {idx + 1}')\n        plt.axis('off')\n\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:26.003681Z","iopub.execute_input":"2025-06-29T10:21:26.003853Z","iopub.status.idle":"2025-06-29T10:21:26.020841Z","shell.execute_reply.started":"2025-06-29T10:21:26.003839Z","shell.execute_reply":"2025-06-29T10:21:26.020151Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# display_keyframes_side_by_side(features[0], augmented_features[60])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:26.021707Z","iopub.execute_input":"2025-06-29T10:21:26.021944Z","iopub.status.idle":"2025-06-29T10:21:26.039337Z","shell.execute_reply.started":"2025-06-29T10:21:26.021921Z","shell.execute_reply":"2025-06-29T10:21:26.038820Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# RUN Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"id":"-Y6FgSVkQdWa","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:26.040044Z","iopub.execute_input":"2025-06-29T10:21:26.040278Z","iopub.status.idle":"2025-06-29T10:21:26.054654Z","shell.execute_reply.started":"2025-06-29T10:21:26.040258Z","shell.execute_reply":"2025-06-29T10:21:26.053793Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# RUN Split the Data into Train ( 75% ) and Test Set ( 25% ).\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features,\n                                                                            one_hot_encoded_labels,\n                                                                            test_size = TEST_SIZE,\n                                                                            shuffle = True,\n                                                                            random_state = seed_constant)","metadata":{"id":"foGv-eppQuRQ","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:26.055371Z","iopub.execute_input":"2025-06-29T10:21:26.055614Z","iopub.status.idle":"2025-06-29T10:21:29.769261Z","shell.execute_reply.started":"2025-06-29T10:21:26.055599Z","shell.execute_reply":"2025-06-29T10:21:29.768503Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Assuming features_train and labels_train are defined\ntrain_video_frames, val_video_frames = features_train[:int(0.8 * len(features_train))], features_train[int(0.8 * len(features_train)):]\ntrain_labels, val_labels = labels_train[:int(0.8 * len(labels_train))], labels_train[int(0.8 * len(labels_train)):]\n\n# Create the training and validation generators\ntrain_gen = VideoDataGenerator(\n    video_frames=train_video_frames, \n    labels=train_labels, \n    batch_size=BATCH_SIZE, \n    #augmentations=[horizontal_flip, random_shear_frames, salt_and_pepper_noise_frames]\n)\n\nval_gen = VideoDataGenerator(\n    video_frames=val_video_frames, \n    labels=val_labels, \n    batch_size=BATCH_SIZE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:29.770069Z","iopub.execute_input":"2025-06-29T10:21:29.770301Z","iopub.status.idle":"2025-06-29T10:21:29.774634Z","shell.execute_reply.started":"2025-06-29T10:21:29.770277Z","shell.execute_reply":"2025-06-29T10:21:29.774088Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"del features_train, labels_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:29.775368Z","iopub.execute_input":"2025-06-29T10:21:29.775600Z","iopub.status.idle":"2025-06-29T10:21:29.795660Z","shell.execute_reply.started":"2025-06-29T10:21:29.775586Z","shell.execute_reply":"2025-06-29T10:21:29.794955Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# # free space in ram memory\n# del augmented_features, augmented_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:29.796385Z","iopub.execute_input":"2025-06-29T10:21:29.796591Z","iopub.status.idle":"2025-06-29T10:21:29.809531Z","shell.execute_reply.started":"2025-06-29T10:21:29.796576Z","shell.execute_reply":"2025-06-29T10:21:29.808919Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import TimeDistributed, Flatten, LSTM, Dropout, Dense\n\ndef create_xception_lstm_model(sequence_length, image_height, image_width, classes_list=None):\n    if classes_list is None:\n        raise ValueError(\"classes_list must be provided to define the output layer size\")\n\n    try:\n        # Load Xception model with pre-trained ImageNet weights\n        print(\"Loading Xception base model...\")\n        xception = Xception(\n            weights='imagenet',\n            include_top=False,\n            input_shape=(image_height, image_width, 3),\n            name=\"Xception\"\n        )\n        # Freeze Xception layers\n        for layer in xception.layers:\n            layer.trainable = False\n        \n        # Define the Sequential model\n        model = Sequential([\n            TimeDistributed(\n                xception,\n                input_shape=(sequence_length, image_height, image_width, 3),\n                name=\"TimeDistributed_Xception\"\n            ),\n            TimeDistributed(Flatten(), name=\"Flatten\"),\n            LSTM(64, activation=\"relu\", return_sequences=False, name=\"LSTM\"),\n            Dropout(0.2, name=\"Dropout\"),\n            Dense(len(classes_list), activation=\"softmax\", name=\"Output\")\n        ])\n        \n        # Print model summary\n        print(\"Model architecture created successfully!\")\n        model.summary()\n\n        return model\n\n    except Exception as e:\n        print(f\"Error creating model: {e}\")\n        return None","metadata":{"id":"0WM_3bdUSE6g","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:29.810320Z","iopub.execute_input":"2025-06-29T10:21:29.810504Z","iopub.status.idle":"2025-06-29T10:21:29.823789Z","shell.execute_reply.started":"2025-06-29T10:21:29.810482Z","shell.execute_reply":"2025-06-29T10:21:29.823205Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# donwload model weights\nfrom tensorflow.keras.applications import Xception\nprint(\"Pre-loading Xception weights...\")\nbase_model = Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\nprint(\"Weights loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:29.824315Z","iopub.execute_input":"2025-06-29T10:21:29.824487Z","iopub.status.idle":"2025-06-29T10:21:36.194868Z","shell.execute_reply.started":"2025-06-29T10:21:29.824453Z","shell.execute_reply":"2025-06-29T10:21:36.194118Z"}},"outputs":[{"name":"stdout","text":"Pre-loading Xception weights...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1751192491.137865      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\nWeights loaded successfully!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# RUN\nimport gc\ntf.keras.backend.clear_session()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:36.195851Z","iopub.execute_input":"2025-06-29T10:21:36.196108Z","iopub.status.idle":"2025-06-29T10:21:36.689349Z","shell.execute_reply.started":"2025-06-29T10:21:36.196086Z","shell.execute_reply":"2025-06-29T10:21:36.688739Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# Clear previous session to free memory\ntf.keras.backend.clear_session()\n\n# Create the model\nxlstm_model = create_xception_lstm_model(\n    sequence_length=SEQUENCE_LENGTH,\n    image_height=IMAGE_HEIGHT,\n    image_width=IMAGE_WIDTH,\n    classes_list=CLASSES_LIST\n)\n\n# Check if model was created successfully\nif xlstm_model is None:\n    print(\"Failed to create model. Check error messages above.\")\nelse:\n    print(\"Model Created Successfully!\")","metadata":{"id":"nWM-_NprSIJx","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:36.690168Z","iopub.execute_input":"2025-06-29T10:21:36.690521Z","iopub.status.idle":"2025-06-29T10:21:38.327287Z","shell.execute_reply.started":"2025-06-29T10:21:36.690486Z","shell.execute_reply":"2025-06-29T10:21:38.326730Z"}},"outputs":[{"name":"stdout","text":"Loading Xception base model...\nModel architecture created successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ TimeDistributed_Xception             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │      \u001b[38;5;34m20,861,480\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Flatten (\u001b[38;5;33mTimeDistributed\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m100352\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ LSTM (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │      \u001b[38;5;34m25,706,752\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m390\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ TimeDistributed_Xception             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">25,706,752</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,568,622\u001b[0m (177.65 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,568,622</span> (177.65 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,707,142\u001b[0m (98.06 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,707,142</span> (98.06 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Model Created Successfully!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Plot the structure of the contructed model.\nplot_model(xlstm_model, to_file = f'{MODEL_NAME}_model_Plot.png', show_shapes = True, show_layer_names = True)\n\nprint(f\"{MODEL_NAME} Model Plot saved successfully...\")","metadata":{"id":"ZViNPAJISIr_","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:38.327970Z","iopub.execute_input":"2025-06-29T10:21:38.328153Z","iopub.status.idle":"2025-06-29T10:21:38.572319Z","shell.execute_reply.started":"2025-06-29T10:21:38.328132Z","shell.execute_reply":"2025-06-29T10:21:38.571596Z"}},"outputs":[{"name":"stdout","text":"Xception Model Plot saved successfully...\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Create an Instance of Early Stopping Callback\nearly_stopping_callback = EarlyStopping(monitor = 'val_loss', \n                                        patience = 7, \n                                        mode = 'min', \n                                        restore_best_weights = True)\n\n# Compile the model and specify loss function, optimizer and metrics values to the model\nxlstm_model.compile(loss = 'categorical_crossentropy', \n                    optimizer = 'Adam', \n                    metrics = [\"accuracy\"])\n\n# Start training the model.\n# = features_train,\n#y = labels_train,\n#batch_size = BATCH_SIZE,\n\nconvlstm_model_training_history = xlstm_model.fit(train_gen,\n                                                  epochs = EPOCHS,\n                                                  shuffle = True,\n                                                  validation_data=val_gen,\n                                                  callbacks = [early_stopping_callback])","metadata":{"id":"dNZIILH5Savo","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T10:21:38.576436Z","iopub.execute_input":"2025-06-29T10:21:38.576684Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1751192569.917952    2496 service.cc:148] XLA service 0x2688c7a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1751192569.919002    2496 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1751192582.319093    2496 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1751192593.381687    2496 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 945ms/step - accuracy: 0.2558 - loss: 201.5883 - val_accuracy: 0.3021 - val_loss: 230.7951\nEpoch 2/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 472ms/step - accuracy: 0.2689 - loss: 241.6668 - val_accuracy: 0.2292 - val_loss: 488.5793\nEpoch 3/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.3002 - loss: 443.9450 - val_accuracy: 0.3438 - val_loss: 721.2109\nEpoch 4/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.3139 - loss: 566.8730 - val_accuracy: 0.4688 - val_loss: 355.7494\nEpoch 5/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 476ms/step - accuracy: 0.3332 - loss: 237.3086 - val_accuracy: 0.3646 - val_loss: 129.7928\nEpoch 6/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.3466 - loss: 156.7970 - val_accuracy: 0.4062 - val_loss: 60.2733\nEpoch 7/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.3961 - loss: 44.5260 - val_accuracy: 0.5417 - val_loss: 19.4283\nEpoch 8/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.4915 - loss: 28.6605 - val_accuracy: 0.5729 - val_loss: 11.6999\nEpoch 9/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.5801 - loss: 18.4930 - val_accuracy: 0.6354 - val_loss: 8.1817\nEpoch 10/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.5201 - loss: 14.6130 - val_accuracy: 0.6042 - val_loss: 7.3645\nEpoch 11/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 475ms/step - accuracy: 0.5730 - loss: 12.0484 - val_accuracy: 0.5521 - val_loss: 7.0963\nEpoch 12/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 475ms/step - accuracy: 0.5580 - loss: 11.8648 - val_accuracy: 0.6250 - val_loss: 5.5981\nEpoch 13/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.5400 - loss: 10.4253 - val_accuracy: 0.6771 - val_loss: 4.3284\nEpoch 14/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 472ms/step - accuracy: 0.6318 - loss: 7.8670 - val_accuracy: 0.6250 - val_loss: 4.4214\nEpoch 15/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 476ms/step - accuracy: 0.6642 - loss: 6.1017 - val_accuracy: 0.6667 - val_loss: 3.3539\nEpoch 16/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 472ms/step - accuracy: 0.5934 - loss: 7.6151 - val_accuracy: 0.6667 - val_loss: 4.0705\nEpoch 17/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.6323 - loss: 6.7458 - val_accuracy: 0.6458 - val_loss: 3.3625\nEpoch 18/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 470ms/step - accuracy: 0.5840 - loss: 5.1208 - val_accuracy: 0.6667 - val_loss: 3.3563\nEpoch 19/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 476ms/step - accuracy: 0.6434 - loss: 5.8338 - val_accuracy: 0.6771 - val_loss: 2.8123\nEpoch 20/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 476ms/step - accuracy: 0.6363 - loss: 4.1424 - val_accuracy: 0.7083 - val_loss: 2.6693\nEpoch 21/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.6598 - loss: 3.4984 - val_accuracy: 0.6667 - val_loss: 2.7056\nEpoch 22/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 476ms/step - accuracy: 0.5926 - loss: 4.4428 - val_accuracy: 0.7604 - val_loss: 2.1215\nEpoch 23/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.6934 - loss: 2.5349 - val_accuracy: 0.7188 - val_loss: 2.6023\nEpoch 24/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 470ms/step - accuracy: 0.6029 - loss: 3.9706 - val_accuracy: 0.7083 - val_loss: 7.7299\nEpoch 25/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 473ms/step - accuracy: 0.6526 - loss: 2.5812 - val_accuracy: 0.6875 - val_loss: 2.6584\nEpoch 26/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 472ms/step - accuracy: 0.6588 - loss: 2.0394 - val_accuracy: 0.6979 - val_loss: 2.3890\nEpoch 27/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.6336 - loss: 2.3517 - val_accuracy: 0.7292 - val_loss: 1.9205\nEpoch 28/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.6716 - loss: 2.2902 - val_accuracy: 0.6979 - val_loss: 1.9603\nEpoch 29/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 477ms/step - accuracy: 0.6626 - loss: 2.8529 - val_accuracy: 0.7292 - val_loss: 1.6773\nEpoch 30/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 475ms/step - accuracy: 0.7139 - loss: 1.8861 - val_accuracy: 0.7292 - val_loss: 1.6464\nEpoch 31/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 475ms/step - accuracy: 0.7527 - loss: 1.7419 - val_accuracy: 0.7292 - val_loss: 1.4768\nEpoch 32/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.7007 - loss: 1.1160 - val_accuracy: 0.7188 - val_loss: 1.4816\nEpoch 33/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 472ms/step - accuracy: 0.6677 - loss: 1.7641 - val_accuracy: 0.6875 - val_loss: 7.0080\nEpoch 34/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 476ms/step - accuracy: 0.6920 - loss: 2.8771 - val_accuracy: 0.6771 - val_loss: 1.2841\nEpoch 35/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 473ms/step - accuracy: 0.7385 - loss: 1.6034 - val_accuracy: 0.6042 - val_loss: 1.5385\nEpoch 36/50\n\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 472ms/step - accuracy: 0.6707 - loss: 1.6849 - val_accuracy: 0.7396 - val_loss: 7.2943\nEpoch 37/50\n\u001b[1m63/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.7340 - loss: 1.5621","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# previous code\nmodel_evaluation_history = xlstm_model.evaluate(features_test, labels_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.title(\"Accuracy\",weight=\"bold\",fontsize=20)\nplt.plot(convlstm_model_training_history.history[\"accuracy\"])\nplt.plot(convlstm_model_training_history.history[\"val_accuracy\"])\nplt.legend([\"Train\",\"Validation\"])\nplt.xlabel(\"Epochs\",weight=\"bold\")\nplt.ylabel(\"Accuracy\",weight=\"bold\")\nplt.subplot(1,2,2)\nplt.title(\"loss\",weight=\"bold\",fontsize=20)\nplt.plot(convlstm_model_training_history.history[\"loss\"])\nplt.plot(convlstm_model_training_history.history[\"val_loss\"])\nplt.legend([\"Train\",\"Validation\"])\nplt.xlabel(\"Epochs\",weight=\"bold\")\nplt.ylabel(\"loss\",weight=\"bold\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate the trained model.\n\n# Create a dataset from your test features and labels\ntest_dataset = tf.data.Dataset.from_tensor_slices((features_test, labels_test))\ntest_dataset = test_dataset.batch(8)  # Try 4, 8, 16 depending on your memory\n\n# Evaluate using the dataset\nmodel_evaluation_history = xlstm_model.evaluate(test_dataset)","metadata":{"id":"9qgXivorTN6F","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Get the loss and accuracy from model_evaluation_history.\n# model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n\n# # Define the string date format.\n# # Get the current Date and Time in a DateTime Object.\n# # Convert the DateTime object to string according to the style mentioned in date_time_format string.\n# date_time_format = '%Y_%m_%d__%H_%M_%S'\n# current_date_time_dt = dt.datetime.now()\n# current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n\n# # Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n# model_file_name = f'xlstm_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n\n# # Save your Model.\n# xlstm_model.save(model_file_name)","metadata":{"id":"zA_1ExNmTWPl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n    '''\n    This function will plot the metrics passed to it in a graph.\n    Args:\n        model_training_history: A history object containing a record of training and validation\n                                loss values and metrics values at successive epochs\n        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n        plot_name:              The title of the graph.\n    '''\n\n    # Get metric values using metric names as identifiers.\n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n\n    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n    epochs = range(len(metric_value_1))\n\n    # Plot the Graph.\n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n\n    # Add title to the plot.\n    plt.title(str(plot_name))\n\n    # Add legend to the plot.\n    plt.legend()","metadata":{"id":"hZiTEY5fUags","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the training and validation loss metrices.\nplot_metric(convlstm_model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')","metadata":{"id":"-8mabamsUexe","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the training and validation accuracy metrices.\nplot_metric(convlstm_model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')","metadata":{"id":"uhJ1tYlUUoNF","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}