{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# install librarires\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:22.565486Z","iopub.execute_input":"2025-07-01T10:25:22.566284Z","iopub.status.idle":"2025-07-01T10:25:22.570373Z","shell.execute_reply.started":"2025-07-01T10:25:22.566257Z","shell.execute_reply":"2025-07-01T10:25:22.569385Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# global variables\n# set dataset directories\n\n\nDATASET_DIR = \"/kaggle/input/msrdailyactivity3d-rgb-videos-only\"\nALL_CLASS_NAMES = os.listdir(DATASET_DIR)\n\n# Global constant variables -> \nNO_OF_CLASSES = 4\nCLASSES_LIST = ALL_CLASS_NAMES[:NO_OF_CLASSES]\n\n# Model Configuration\nIMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\nSEQUENCE_LENGTH = 15\n\n# set drop out rate\nDROPOUT_RATE = 0.3\n\n# set datas\nMAX_VIDEO_PER_CLASS = 5\n\n# split dataset\nTEST_SIZE = 0.20\n\n# model fit parameters\nEPOCHS = 50\nBATCH_SIZE = 4\nVALIDATION_SPLIT = 0.20\n\n\n# give a name of the model to save\nMODEL_NAME = \"Xception\"\n\nprint(f\"There are total {len(ALL_CLASS_NAMES)} classes, selected {NO_OF_CLASSES} classes\")\nprint(f\"Setting {MAX_VIDEO_PER_CLASS}/class to train the model.\")\nprint(f\"Image size {IMAGE_HEIGHT}x{IMAGE_WIDTH} with {SEQUENCE_LENGTH} sequence length\")\nprint(f\"Dropout rate: {DROPOUT_RATE}\")\nprint(f\"Train-Test split ratio {int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}\")\nprint(f\"Validation data from Train set {VALIDATION_SPLIT*100}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:22.571714Z","iopub.execute_input":"2025-07-01T10:25:22.572069Z","iopub.status.idle":"2025-07-01T10:25:22.594736Z","shell.execute_reply.started":"2025-07-01T10:25:22.572015Z","shell.execute_reply":"2025-07-01T10:25:22.593579Z"}},"outputs":[{"name":"stdout","text":"There are total 16 classes, selected 4 classes\nSetting 5/class to train the model.\nImage size 128x128 with 15 sequence length\nDropout rate: 0.3\nTrain-Test split ratio 80/20\nValidation data from Train set 20.0%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# set seeed to get similar values\nseed_constant = 27\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:22.595624Z","iopub.execute_input":"2025-07-01T10:25:22.595936Z","iopub.status.idle":"2025-07-01T10:25:22.609141Z","shell.execute_reply.started":"2025-07-01T10:25:22.595909Z","shell.execute_reply":"2025-07-01T10:25:22.608122Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import cv2\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef resize_and_normalize_frame(frame, image_height, image_width):\n    try:\n        resized_frame = cv2.resize(frame, (image_width, image_height), interpolation=cv2.INTER_LINEAR)\n        normalized_frame = resized_frame / 255.0\n        return normalized_frame\n    except Exception as e:\n        print(f\"Error processing frame: {e}\")\n        return None\n\ndef frames_extraction(video_path, \n                      sequence_length=SEQUENCE_LENGTH, \n                      image_height=IMAGE_HEIGHT, \n                      image_width=IMAGE_WIDTH):\n    # Declare a list to store video frames\n    frames_list = []\n\n    # Check if video file exists\n    if not os.path.exists(video_path):\n        print(f\"Error: Video file not found at {video_path}\")\n        return None\n\n    # Read the video file using VideoCapture with optimized settings\n    video_reader = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)\n\n    # Check if the video was opened successfully\n    if not video_reader.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        video_reader.release()\n        return None\n\n    # Get the total number of frames in the video\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Ensure the video has enough frames\n    if video_frames_count < sequence_length:\n        print(f\"Warning: Video {video_path} has only {video_frames_count} frames, less than required {sequence_length}\")\n        video_reader.release()\n        return None\n\n    # Calculate the interval after which frames will be sampled\n    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n\n    # Pre-allocate frame indices to extract\n    frame_indices = [i * skip_frames_window for i in range(sequence_length)]\n\n    # Read and process frames in parallel\n    frames = []\n    for idx in frame_indices:\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        success, frame = video_reader.read()\n        if not success or frame is None:\n            print(f\"Warning: Failed to read frame at index {idx} from {video_path}\")\n            break\n        frames.append(frame)\n\n    # Release the VideoCapture object early\n    video_reader.release()\n\n    # Ensure the correct number of frames is read\n    if len(frames) != sequence_length:\n        print(f\"Warning: Read {len(frames)} frames instead of {sequence_length} from {video_path}\")\n        return None\n\n    # Process frames in parallel using ThreadPoolExecutor\n    with ThreadPoolExecutor() as executor:\n        processed_frames = list(executor.map(\n            lambda f: resize_and_normalize_frame(f, image_height, image_width), \n            frames\n        ))\n\n    # Check for any failed frame processing\n    if any(f is None for f in processed_frames):\n        print(f\"Warning: Some frames failed to process in {video_path}\")\n        return None\n\n    # Convert to NumPy array\n    frames_array = np.array(processed_frames, dtype=np.float32)\n\n    return frames_array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:22.610670Z","iopub.execute_input":"2025-07-01T10:25:22.610984Z","iopub.status.idle":"2025-07-01T10:25:22.631004Z","shell.execute_reply.started":"2025-07-01T10:25:22.610957Z","shell.execute_reply":"2025-07-01T10:25:22.630146Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# RUN create dataset function definition\ndef create_dataset(dataset_dir,\n                   classes_list, \n                   sequence_length=SEQUENCE_LENGTH, \n                   image_height=IMAGE_HEIGHT, \n                   image_width=IMAGE_WIDTH, \n                   max_videos_per_class=None,\n                   augmentations=False\n                  ):\n    \n    # Initialize lists to store features, labels, and video file paths\n    features = []\n    labels = []\n\n    # Check if dataset directory exists\n    if not os.path.exists(dataset_dir):\n        raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n\n    # Iterate through all classes in the classes list\n    for class_index, class_name in enumerate(classes_list):\n        class_path = os.path.join(dataset_dir, class_name)\n        \n        # Check if class directory exists\n        if not os.path.exists(class_path):\n            print(f\"Warning: Class directory not found: {class_path}\")\n            continue\n\n        print(f'Extracting Data of Class: {class_name}')\n\n        # Get the list of video files in the class directory\n        files_list = os.listdir(class_path)\n\n        # Limit the number of videos if specified\n        if max_videos_per_class is not None:\n            files_list = files_list[:max_videos_per_class]\n\n        # Iterate through all video files\n        for file_name in files_list:\n            video_file_path = os.path.join(class_path, file_name)\n\n            # Extract frames using the updated frames_extraction function\n            frames = frames_extraction(video_file_path, sequence_length, image_height, image_width)\n\n            # Skip videos where frame extraction failed\n            if frames is None:\n                print(f\"Skipping video {video_file_path} due to frame extraction failure\")\n                continue\n\n            # Append the data to respective lists\n            features.append(frames)\n            labels.append(class_index)\n\n    # Convert lists to numpy arrays\n    if not features:\n        raise ValueError(\"No valid videos were processed. Check dataset or parameters.\")\n    features = np.asarray(features)\n    labels = np.array(labels)\n\n    print(f\"Dataset created with {len(features)} videos\")\n    print(f\"Features shape: {features.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n\n    return features, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:22.781186Z","iopub.execute_input":"2025-07-01T10:25:22.781497Z","iopub.status.idle":"2025-07-01T10:25:22.792544Z","shell.execute_reply.started":"2025-07-01T10:25:22.781474Z","shell.execute_reply":"2025-07-01T10:25:22.791564Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# RUN Create the dataset with explicit parameters\ntry:\n    features, labels = create_dataset(\n        dataset_dir=DATASET_DIR,\n        classes_list=CLASSES_LIST,\n        sequence_length=SEQUENCE_LENGTH,\n        image_height=IMAGE_HEIGHT,\n        image_width=IMAGE_WIDTH,\n        # Limit to 10 videos per class to manage memory\n        augmentations=True,\n        max_videos_per_class=MAX_VIDEO_PER_CLASS\n    )\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n\nfeatures.shape, labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:22.794104Z","iopub.execute_input":"2025-07-01T10:25:22.794871Z","iopub.status.idle":"2025-07-01T10:25:28.625755Z","shell.execute_reply.started":"2025-07-01T10:25:22.794837Z","shell.execute_reply":"2025-07-01T10:25:28.624808Z"}},"outputs":[{"name":"stdout","text":"Extracting Data of Class: write on a paper\nExtracting Data of Class: use laptop\nExtracting Data of Class: read book\nExtracting Data of Class: sit still\nDataset created with 20 videos\nFeatures shape: (20, 15, 128, 128, 3)\nLabels shape: (20,)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"((20, 15, 128, 128, 3), (20,))"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence\nimport albumentations as A\n\nclass VideoDataGenerator(Sequence):\n    def __init__(self, video_frames, labels, batch_size, augmentations=False):\n        self.video_frames = video_frames  # List of sequences, shape: (sequence_length, height, width, channels)\n        self.labels = labels  # List or array of labels\n        self.batch_size = batch_size\n        self.indices = np.arange(len(self.video_frames))  # For shuffling\n\n        # Convert inputs to NumPy arrays, assuming frames are already normalized to [0, 1]\n        self.video_frames = [np.array(seq, dtype=np.float32) for seq in self.video_frames]\n        self.labels = np.array(self.labels)\n\n        # Define augmentation pipelines using albumentations\n        self.random_shear = A.Compose([\n            A.Affine(shear={'x': (-10, 10), 'y': (-10, 10)}, mode='constant', cval=0, p=1.0)\n        ])\n        self.horizontal_flip = A.Compose([\n            A.HorizontalFlip(p=1.0)\n        ])\n        self.salt_pepper_noise = A.Compose([\n            A.GaussNoise(var_limit=(0.001, 0.005), mean=0, p=1.0)  # Proxy for salt-and-pepper noise\n        ])\n\n    def __len__(self):\n        # Return the number of batches per epoch\n        return int(np.ceil(len(self.video_frames) / self.batch_size))\n\n    def __getitem__(self, idx):\n        # Get batch indices\n        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        # Handle case where batch_indices is empty\n        if len(batch_indices) == 0:\n            return np.array([], dtype=np.float32), np.array([], dtype=np.float32)\n\n        # Select batch data\n        batch_video_frames = [self.video_frames[i] for i in batch_indices]\n        batch_labels = self.labels[batch_indices]\n\n        # Initialize lists to store original and augmented frames\n        all_frames = []\n        all_labels = []\n        # Process each sequence in the batch\n        for i, sequence in enumerate(batch_video_frames):\n            # Original frames\n            all_frames.append(sequence)\n            all_labels.append(batch_labels[i])\n\n            # Apply each augmentation separately\n            for aug in [self.random_shear, self.horizontal_flip, self.salt_pepper_noise]:\n                # Apply augmentation to each frame in the sequence\n                aug_sequence = np.zeros_like(sequence)\n                for t in range(sequence.shape[0]):\n                    aug_frame = aug(image=sequence[t])['image']\n                    aug_sequence[t] = aug_frame\n                all_frames.append(aug_sequence)\n                all_labels.append(batch_labels[i])\n\n        # Convert to numpy arrays\n        all_frames = np.array(all_frames, dtype=np.float32)\n        all_labels = np.array(all_labels)\n\n        return all_frames, all_labels\n\n    def on_epoch_end(self):\n        # Shuffle indices at the end of each epoch\n        np.random.shuffle(self.indices)\n\n    def as_dataset(self):\n        def generator():\n            for idx in range(len(self)):\n                frames, labels = self[idx]\n                # Skip empty batches\n                if frames.size == 0:\n                    continue\n                # Yield batches with shape (batch_size * 4, sequence_length, height, width, channels)\n                yield frames, labels\n\n        # Create a tf.data.Dataset\n        dataset = tf.data.Dataset.from_generator(\n            generator,\n            output_types=(tf.float32, tf.float32),\n            output_shapes=(\n                (None, self.video_frames[0].shape[0], self.video_frames[0].shape[1], \n                 self.video_frames[0].shape[2], self.video_frames[0].shape[3]),\n                (None,) + self.labels.shape[1:]\n            )\n        )\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:28.626623Z","iopub.execute_input":"2025-07-01T10:25:28.626972Z","iopub.status.idle":"2025-07-01T10:25:28.643213Z","shell.execute_reply.started":"2025-07-01T10:25:28.626944Z","shell.execute_reply":"2025-07-01T10:25:28.641992Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n# RUN Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:28.644274Z","iopub.execute_input":"2025-07-01T10:25:28.644593Z","iopub.status.idle":"2025-07-01T10:25:28.669926Z","shell.execute_reply.started":"2025-07-01T10:25:28.644570Z","shell.execute_reply":"2025-07-01T10:25:28.668754Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# RUN Split the Data into Train ( 75% ) and Test Set ( 25% ).\nfrom sklearn.model_selection import train_test_split\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features,\n                                                                            one_hot_encoded_labels,\n                                                                            test_size = TEST_SIZE,\n                                                                            shuffle = True,\n                                                                            random_state = seed_constant)\n\nfeatures_train.shape, labels_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:28.672293Z","iopub.execute_input":"2025-07-01T10:25:28.672578Z","iopub.status.idle":"2025-07-01T10:25:28.719174Z","shell.execute_reply.started":"2025-07-01T10:25:28.672558Z","shell.execute_reply":"2025-07-01T10:25:28.718189Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"((16, 15, 128, 128, 3), (16, 4))"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Assuming features_train and labels_train are defined\ntrain_video_frames, val_video_frames = features_train[:int(0.8 * len(features_train))], features_train[int(0.8 * len(features_train)):]\ntrain_labels, val_labels = labels_train[:int(0.8 * len(labels_train))], labels_train[int(0.8 * len(labels_train)):]\n\ntrain_video_frames.shape, val_video_frames.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:28.720111Z","iopub.execute_input":"2025-07-01T10:25:28.720426Z","iopub.status.idle":"2025-07-01T10:25:28.728136Z","shell.execute_reply.started":"2025-07-01T10:25:28.720398Z","shell.execute_reply":"2025-07-01T10:25:28.727297Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"((12, 15, 128, 128, 3), (4, 15, 128, 128, 3))"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Create the training and validation generators\ntrain_gen = VideoDataGenerator(\n    video_frames=train_video_frames, \n    labels=train_labels, \n    batch_size=BATCH_SIZE, \n    augmentations=False\n)\n\nval_gen = VideoDataGenerator(\n    video_frames=val_video_frames, \n    labels=val_labels, \n    batch_size=BATCH_SIZE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:28.729064Z","iopub.execute_input":"2025-07-01T10:25:28.729551Z","iopub.status.idle":"2025-07-01T10:25:28.791120Z","shell.execute_reply.started":"2025-07-01T10:25:28.729520Z","shell.execute_reply":"2025-07-01T10:25:28.790192Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/691017718.py:17: UserWarning: Argument(s) 'mode, cval' are not valid for transform Affine\n  A.Affine(shear={'x': (-10, 10), 'y': (-10, 10)}, mode='constant', cval=0, p=1.0)\n/tmp/ipykernel_35/691017718.py:23: UserWarning: Argument(s) 'var_limit, mean' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(0.001, 0.005), mean=0, p=1.0)  # Proxy for salt-and-pepper noise\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# delele to free memory\n#del features_train, labels_train, train_video_frames, train_labels, val_video_frames, val_labels, augmented_features, augmented_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:25:28.792038Z","iopub.execute_input":"2025-07-01T10:25:28.792284Z","iopub.status.idle":"2025-07-01T10:25:28.796524Z","shell.execute_reply.started":"2025-07-01T10:25:28.792265Z","shell.execute_reply":"2025-07-01T10:25:28.795628Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import TimeDistributed, Flatten, LSTM, Dropout, Dense\n\ndef create_xception_lstm_model(sequence_length, image_height, image_width, classes_list=None):\n    if classes_list is None:\n        raise ValueError(\"classes_list must be provided to define the output layer size\")\n\n    try:\n        # Load Xception model with pre-trained ImageNet weights\n        print(\"Loading Xception base model...\")\n        xception = Xception(\n            weights='imagenet',\n            include_top=False,\n            input_shape=(image_height, image_width, 3),\n            name=\"Xception\"\n        )\n        # Freeze Xception layers\n        for layer in xception.layers:\n            layer.trainable = False\n\n        for layer in xception.layers[-5:]:\n            layer.trainable = True\n        \n        # Define the Sequential model\n        model = Sequential([\n            TimeDistributed(\n                xception,\n                input_shape=(sequence_length, image_height, image_width, 3),\n                name=\"TimeDistributed_Xception\"\n            ),\n            TimeDistributed(GlobalAveragePooling2D(), name=\"global_avg_pooling\"),  # Reduces to (sequence_length, 2048)\n            LSTM(128, activation=\"tanh\", return_sequences=False, name=\"LSTM\"),\n            Dropout(DROPOUT_RATE, name=\"Dropout\"),\n            Dense(len(classes_list), activation=\"softmax\", name=\"Output\")\n        ])\n        \n        # Print model summary\n        print(\"Model architecture created successfully!\")\n        model.summary()\n\n        return model\n\n    except Exception as e:\n        print(f\"Error creating model: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:27:44.099849Z","iopub.execute_input":"2025-07-01T10:27:44.100181Z","iopub.status.idle":"2025-07-01T10:27:44.110490Z","shell.execute_reply.started":"2025-07-01T10:27:44.100157Z","shell.execute_reply":"2025-07-01T10:27:44.109583Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# donwload model weights\nfrom tensorflow.keras.applications import Xception\nprint(\"Pre-loading Xception weights...\")\nbase_model = Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\nprint(\"Weights loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:27:44.112044Z","iopub.execute_input":"2025-07-01T10:27:44.112385Z","iopub.status.idle":"2025-07-01T10:27:45.295482Z","shell.execute_reply.started":"2025-07-01T10:27:44.112356Z","shell.execute_reply":"2025-07-01T10:27:45.294558Z"}},"outputs":[{"name":"stdout","text":"Pre-loading Xception weights...\nWeights loaded successfully!\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Clear previous session to free memory\ntf.keras.backend.clear_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:27:45.296674Z","iopub.execute_input":"2025-07-01T10:27:45.297637Z","iopub.status.idle":"2025-07-01T10:27:45.699913Z","shell.execute_reply.started":"2025-07-01T10:27:45.297586Z","shell.execute_reply":"2025-07-01T10:27:45.698387Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Create the model\nxlstm_model = create_xception_lstm_model(\n    sequence_length=SEQUENCE_LENGTH,\n    image_height=IMAGE_HEIGHT,\n    image_width=IMAGE_WIDTH,\n    classes_list=CLASSES_LIST\n)\n\n# Check if model was created successfully\nif xlstm_model is None:\n    print(\"Failed to create model. Check error messages above.\")\nelse:\n    print(\"Model Created Successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:27:45.704582Z","iopub.execute_input":"2025-07-01T10:27:45.704948Z","iopub.status.idle":"2025-07-01T10:27:47.193262Z","shell.execute_reply.started":"2025-07-01T10:27:45.704921Z","shell.execute_reply":"2025-07-01T10:27:47.192371Z"}},"outputs":[{"name":"stdout","text":"Loading Xception base model...\nModel architecture created successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ TimeDistributed_Xception             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │      \u001b[38;5;34m20,861,480\u001b[0m │\n│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_avg_pooling (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m2048\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ LSTM (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m1,114,624\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m516\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ TimeDistributed_Xception             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_avg_pooling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,114,624</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,976,620\u001b[0m (83.83 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,976,620</span> (83.83 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,281,860\u001b[0m (16.33 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,281,860</span> (16.33 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,694,760\u001b[0m (67.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,694,760</span> (67.50 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Model Created Successfully!\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Plot the structure of the contructed model.\nfrom tensorflow.keras.utils import plot_model\n\nplot_model(xlstm_model, to_file = f'{MODEL_NAME}_model_Plot.png', show_shapes = True, show_layer_names = True)\n\nprint(f\"{MODEL_NAME} Model Plot saved successfully...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:27:47.194281Z","iopub.execute_input":"2025-07-01T10:27:47.194756Z","iopub.status.idle":"2025-07-01T10:27:47.480041Z","shell.execute_reply.started":"2025-07-01T10:27:47.194726Z","shell.execute_reply":"2025-07-01T10:27:47.479145Z"}},"outputs":[{"name":"stdout","text":"Xception Model Plot saved successfully...\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Create an Instance of Early Stopping Callback\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping_callback = EarlyStopping(monitor = 'val_loss', \n                                        patience = 7, \n                                        mode = 'min', \n                                        restore_best_weights = True)\n\n# Compile the model and specify loss function, optimizer and metrics values to the model\nxlstm_model.compile(loss = 'categorical_crossentropy', \n                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                    metrics = [\"accuracy\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:27:47.481025Z","iopub.execute_input":"2025-07-01T10:27:47.481247Z","iopub.status.idle":"2025-07-01T10:27:47.501719Z","shell.execute_reply.started":"2025-07-01T10:27:47.481230Z","shell.execute_reply":"2025-07-01T10:27:47.500785Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', \n                                 factor=0.5, \n                                 patience=3, \n                                 min_lr=1e-6)\n\n\nxlstm_model.fit(\n    train_gen.as_dataset(),\n    epochs=EPOCHS,\n    validation_data=val_gen.as_dataset(),\n    callbacks=[early_stopping_callback, lr_scheduler]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:27:47.502591Z","iopub.execute_input":"2025-07-01T10:27:47.502945Z","iopub.status.idle":"2025-07-01T10:34:14.658421Z","shell.execute_reply.started":"2025-07-01T10:27:47.502916Z","shell.execute_reply":"2025-07-01T10:34:14.657637Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n      3/Unknown \u001b[1m122s\u001b[0m 8s/step - accuracy: 0.4028 - loss: 1.3713","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 24s/step - accuracy: 0.3854 - loss: 1.3796 - val_accuracy: 0.2500 - val_loss: 1.3477 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 12s/step - accuracy: 0.5573 - loss: 1.1650 - val_accuracy: 0.1250 - val_loss: 1.4982 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 12s/step - accuracy: 0.6771 - loss: 1.0191 - val_accuracy: 0.1875 - val_loss: 1.5701 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 12s/step - accuracy: 0.7734 - loss: 0.9485 - val_accuracy: 0.1875 - val_loss: 1.5718 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 12s/step - accuracy: 0.7526 - loss: 0.8862 - val_accuracy: 0.1875 - val_loss: 1.5813 - learning_rate: 5.0000e-05\nEpoch 6/50\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 12s/step - accuracy: 0.7500 - loss: 0.8577 - val_accuracy: 0.2500 - val_loss: 1.5196 - learning_rate: 5.0000e-05\nEpoch 7/50\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 11s/step - accuracy: 0.7812 - loss: 0.7986 - val_accuracy: 0.2500 - val_loss: 1.5205 - learning_rate: 5.0000e-05\nEpoch 8/50\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - accuracy: 0.7552 - loss: 0.7961 - val_accuracy: 0.2500 - val_loss: 1.4961 - learning_rate: 2.5000e-05\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ac6b1c9e450>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# previous code\nmodel_evaluation_history = xlstm_model.evaluate(features_test, labels_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:34:14.659311Z","iopub.execute_input":"2025-07-01T10:34:14.659559Z","iopub.status.idle":"2025-07-01T10:34:17.324621Z","shell.execute_reply.started":"2025-07-01T10:34:14.659539Z","shell.execute_reply":"2025-07-01T10:34:17.323689Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2500 - loss: 1.4729\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Create test generator with batch_size matching test set size\ntest_gen = VideoDataGenerator(\n    video_frames=features_test,\n    labels=labels_test,\n    batch_size=4,  # Process all test samples at once\n    augmentations=False\n)\n\n# Get predictions\npredictions = xlstm_model.predict(test_gen.as_dataset())\npredicted_classes = np.argmax(predictions, axis=1)\ntrue_classes = np.argmax(labels_test, axis=1)\n\n# Verify shapes\nprint(f\"True classes shape: {true_classes.shape}\")\nprint(f\"Predicted classes shape: {predicted_classes.shape}\")\nassert len(true_classes) == len(predicted_classes), \"Sample counts do not match!\"\n\n# Generate classification report\nprint(\"Classification Report:\")\nprint(classification_report(true_classes, predicted_classes, target_names=CLASSES_LIST))\n\n# Plot confusion matrix\ncm = confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES_LIST, yticklabels=CLASSES_LIST)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T10:34:17.326082Z","iopub.execute_input":"2025-07-01T10:34:17.326413Z","iopub.status.idle":"2025-07-01T10:34:51.010992Z","shell.execute_reply.started":"2025-07-01T10:34:17.326391Z","shell.execute_reply":"2025-07-01T10:34:51.009787Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/691017718.py:17: UserWarning: Argument(s) 'mode, cval' are not valid for transform Affine\n  A.Affine(shear={'x': (-10, 10), 'y': (-10, 10)}, mode='constant', cval=0, p=1.0)\n/tmp/ipykernel_35/691017718.py:23: UserWarning: Argument(s) 'var_limit, mean' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(0.001, 0.005), mean=0, p=1.0)  # Proxy for salt-and-pepper noise\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 32s/step\nTrue classes shape: (4,)\nPredicted classes shape: (16,)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self._interrupted_warning()\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3099934956.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"True classes shape: {true_classes.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicted classes shape: {predicted_classes.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sample counts do not match!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Generate classification report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Sample counts do not match!"],"ename":"AssertionError","evalue":"Sample counts do not match!","output_type":"error"}],"execution_count":43}]}