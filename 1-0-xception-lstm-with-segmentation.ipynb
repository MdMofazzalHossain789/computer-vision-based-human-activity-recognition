{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2256095,"sourceType":"datasetVersion","datasetId":1357563},{"sourceId":12338519,"sourceType":"datasetVersion","datasetId":7763484},{"sourceId":12373562,"sourceType":"datasetVersion","datasetId":7801884}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initialization\n## Import Libraries","metadata":{}},{"cell_type":"code","source":"pip install mediapipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:22.626289Z","iopub.execute_input":"2025-07-06T04:37:22.626770Z","iopub.status.idle":"2025-07-06T04:37:25.626137Z","shell.execute_reply.started":"2025-07-06T04:37:22.626746Z","shell.execute_reply":"2025-07-06T04:37:25.625337Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\nRequirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\nRequirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.7.2)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\nRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\nRequirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.8)\nRequirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->mediapipe) (2.4.1)\nRequirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\nRequirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\nRequirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\nRequirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mediapipe) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mediapipe) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->mediapipe) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->mediapipe) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->mediapipe) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":218},{"cell_type":"code","source":"# install librarires\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.627730Z","iopub.execute_input":"2025-07-06T04:37:25.627976Z","iopub.status.idle":"2025-07-06T04:37:25.632264Z","shell.execute_reply.started":"2025-07-06T04:37:25.627956Z","shell.execute_reply":"2025-07-06T04:37:25.631358Z"}},"outputs":[],"execution_count":219},{"cell_type":"markdown","source":"## ‚ÑπÔ∏è Set Global Constants","metadata":{}},{"cell_type":"code","source":"# global variables\n# set dataset directories\n\n\nDATASET_DIR = \"/kaggle/input/msrdailyactivity3d-rgb-videos-only\"\nDATASET_NAME = DATASET_DIR.split(\"/\")[-1].replace(\"-\", \" \").title()\n\nprint(DATASET_NAME)\n\nALL_CLASS_NAMES = os.listdir(DATASET_DIR)\n\n# Global constant variables -> \nNO_OF_CLASSES = 16\nCLASSES_LIST = ALL_CLASS_NAMES[:NO_OF_CLASSES]\n\n# Model Configuration\nIMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\nSEQUENCE_LENGTH = 5\n\n# set drop out rate\nDROPOUT_RATE = 0.5\n\n# set datas\nMAX_VIDEO_PER_CLASS = 20\n\n# split dataset\nTEST_SIZE = 0.20\n\n# model fit parameters\nEPOCHS = 30\nBATCH_SIZE = 16\nVALIDATION_SPLIT = 0.20\n\n# augmentation values\nNOISE_FACTOR = 0.02\nSHEAR_X= 1\nSHEAR_Y= 1\n\n# give a name of the model to save\nMODEL_NAME = \"Xception\"\n\nprint(f\"There are total {len(ALL_CLASS_NAMES)} classes, selected {NO_OF_CLASSES} classes\")\nprint(f\"Setting {MAX_VIDEO_PER_CLASS}/class to train the model.\")\nprint(f\"Image size {IMAGE_HEIGHT}x{IMAGE_WIDTH} with {SEQUENCE_LENGTH} sequence length\")\nprint(f\"Dropout rate: {DROPOUT_RATE}\")\nprint(f\"Train-Test split ratio {int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}\")\nprint(f\"Validation data from Train set {VALIDATION_SPLIT*100}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.633064Z","iopub.execute_input":"2025-07-06T04:37:25.633286Z","iopub.status.idle":"2025-07-06T04:37:25.650327Z","shell.execute_reply.started":"2025-07-06T04:37:25.633262Z","shell.execute_reply":"2025-07-06T04:37:25.649725Z"}},"outputs":[{"name":"stdout","text":"Msrdailyactivity3D Rgb Videos Only\nThere are total 16 classes, selected 16 classes\nSetting 20/class to train the model.\nImage size 128x128 with 5 sequence length\nDropout rate: 0.5\nTrain-Test split ratio 80/20\nValidation data from Train set 20.0%\n","output_type":"stream"}],"execution_count":220},{"cell_type":"markdown","source":"## Set `Seed` Values","metadata":{}},{"cell_type":"code","source":"# set seeed to get similar values\nseed_constant = 27\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.652048Z","iopub.execute_input":"2025-07-06T04:37:25.652256Z","iopub.status.idle":"2025-07-06T04:37:25.675936Z","shell.execute_reply.started":"2025-07-06T04:37:25.652241Z","shell.execute_reply":"2025-07-06T04:37:25.675328Z"}},"outputs":[],"execution_count":221},{"cell_type":"markdown","source":"# üìå Function Definition\n## Resize and Frame Extraction","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nimport cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom skimage.metrics import structural_similarity as ssim\n\n# Placeholder for kmeans_silhouette function\ndef kmeans_silhouette(sub_features):\n    if len(sub_features) < 2:\n        return [], [], 1, [0]  # Return empty results for insufficient frames\n    \n    # Try different k values (2 to min(5, len(sub_features)))\n    best_k = 2\n    best_labels = None\n    best_centers = None\n    best_score = -1\n    best_indices = []\n    \n    for k in range(2, min(6, len(sub_features) + 1)):\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n        labels = kmeans.fit_predict(sub_features)\n        if len(np.unique(labels)) > 1:  # Silhouette score requires at least 2 clusters\n            score = silhouette_score(sub_features, labels)\n            if score > best_score:\n                best_score = score\n                best_k = k\n                best_labels = labels\n                best_centers = kmeans.cluster_centers_\n        \n        # Find indices of frames closest to cluster centers\n        indices = []\n        for i in range(best_k):\n            if i in labels:\n                cluster_points = sub_features[labels == i]\n                distances = np.linalg.norm(cluster_points - best_centers[i], axis=1)\n                if len(distances) > 0:\n                    idx = np.argmin(distances)\n                    indices.append(idx)\n    \n    return best_labels, best_centers, best_k, indices\n\n# Placeholder for redundancy function\ndef redundancy(video_path, indices, threshold=0.94):\n    if not indices:\n        return []\n    \n    video_reader = cv2.VideoCapture(video_path)\n    if not video_reader.isOpened():\n        return indices\n    \n    # Compute histogram for each frame\n    def get_histogram(frame):\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n        cv2.normalize(hist, hist)\n        return hist\n    \n    filtered_indices = [indices[0]]  # Keep first index\n    prev_frame = None\n    \n    for idx in indices[1:]:\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        success, frame = video_reader.read()\n        if not success:\n            continue\n        hist = get_histogram(frame)\n        \n        if prev_frame is not None:\n            # Compute histogram correlation\n            corr = cv2.compareHist(get_histogram(prev_frame), hist, cv2.HISTCMP_CORREL)\n            if corr < threshold:  # Keep frame if significantly different\n                filtered_indices.append(idx)\n        prev_frame = frame\n    \n    video_reader.release()\n    return filtered_indices\n\n# Modified scen_keyframe_extraction function\ndef scen_keyframe_extraction(video_path,  file_name, video_index, is_last_video, num_segments=3):\n    # Open video to get frame count\n    video_reader = cv2.VideoCapture(video_path)\n    if not video_reader.isOpened():\n        raise ValueError(f\"Failed to open video {video_path}.\")\n    \n    frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    if frame_count == 0:\n        video_reader.release()\n        raise ValueError(f\"No frames in video {video_path}.\")\n    \n    # Generate synthetic scene segmentation (divide video into equal segments)\n    segment_length = frame_count // num_segments\n    number_list = []\n    for i in range(num_segments):\n        start = i * segment_length\n        end = min((i + 1) * segment_length, frame_count)\n        number_list.extend([start, end])\n\n    features = []\n    frame_idx = 0\n    \n    while True:\n        success, frame = video_reader.read()\n        if not success:\n            break\n    \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n        cv2.normalize(hist, hist)\n        features.append(hist.flatten())\n        \n        frame_idx += 1\n    \n    video_reader.release()\n    features = np.asarray(features)\n    \n    # Clustering at each shot to obtain keyframe sequence numbers\n    keyframe_index = []\n    for i in range(0, len(number_list), 2):\n        start = number_list[i]\n        end = min(number_list[i + 1], len(features))\n        if end <= start:\n            continue\n        sub_features = features[start:end]\n        best_labels, best_centers, k, indices = kmeans_silhouette(sub_features)\n        final_index = [x + start for x in indices if x + start < frame_count]\n        final_index = redundancy(video_path, final_index, 0.94)\n        keyframe_index += final_index\n    \n    keyframe_index = sorted(list(set(keyframe_index)))  # Remove duplicates and sort\n    # print(\"final_index:\", keyframe_index, end=\"\\r\", flush=True)\n    \n    # Visualize keyframes\n    num_keyframes = len(keyframe_index)\n    if num_keyframes == 0:\n        print(\"No keyframes extracted.\")\n        return\n    \n    video_reader = cv2.VideoCapture(video_path)\n    class_name = os.path.basename(os.path.dirname(video_path))\n    # print(keyframe_index)\n    frames=[]\n    frames_index=[]\n    for idx, frame_idx in enumerate(keyframe_index, 1):\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        success, frame = video_reader.read()\n        if not success:\n            continue\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        try:\n            resized_frame = cv2.resize(rgb_frame, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_AREA)\n            normalized_frame = resized_frame.astype('float32') / 255.0\n\n            \n            if normalized_frame.shape != (IMAGE_HEIGHT, IMAGE_WIDTH, 3):\n                print(f\"Warning: Frame at index {frame_idx} has incorrect shape: {normalized_frame.shape}\")\n                continue  # Skip malformed frame\n            if is_last_video and idx == num_keyframes:\n                print(f\"Extracted {len(frames)+1} frames from file {video_index+1}: {file_name}\", end=\"\\n\", flush=True)\n            else:\n                print(f\"Extracted {len(frames)+1} frames from file {video_index+1}: {file_name}\", end=\"\\r\", flush=True)\n                \n            frames.append(normalized_frame)\n            frames_index.append(frame_idx)\n        \n        except Exception as e:\n            print(f\"Error processing frame {frame_idx}: {e}\")\n            continue\n    video_reader.release()\n\n    # handle inhomogenous frames or shape mismatch\n    if len(frames) > SEQUENCE_LENGTH:\n        # if it is greater than sequence length then leave the last frames\n        frames=frames[:SEQUENCE_LENGTH]\n    \n    frames=np.asarray(frames)\n    frames_index=np.asarray(frames_index)\n    return frames, frames_index\n\n\ndef show_frames(frames, index):\n    num_keyframes = len(frames)\n    plt.figure(figsize=(5 * num_keyframes, 5))\n\n    for plot_index, (i, frame) in enumerate(zip(index, frames), start=1):\n        frame_label = f\"(Frame {i})\"\n        frame = frame.copy()  # Ensure cv2.putText doesn't affect original\n        cv2.putText(frame, frame_label, (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n        plt.subplot(1, num_keyframes, plot_index)\n        plt.imshow(frame)\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.savefig('ucf50_biking_keyframe_summary.jpg', bbox_inches='tight', dpi=300)\n    plt.show()\n\n    \n# # Run the keyframe extraction\n# sequence, index = scen_keyframe_extraction(video_path, num_segments=5)\n\n# print(sequence.shape)\n\n# show_frames(sequence, index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.680755Z","iopub.execute_input":"2025-07-06T04:37:25.680957Z","iopub.status.idle":"2025-07-06T04:37:25.702048Z","shell.execute_reply.started":"2025-07-06T04:37:25.680938Z","shell.execute_reply":"2025-07-06T04:37:25.701499Z"}},"outputs":[],"execution_count":222},{"cell_type":"markdown","source":"### ‚ùå DEPRECIATED Resize and Frame Extraction","metadata":{}},{"cell_type":"code","source":"# Resize and Frame Extraction\nimport cv2\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef resize_and_normalize_frame(frame, image_height, image_width):\n    try:\n        resized_frame = cv2.resize(frame, (image_width, image_height), interpolation=cv2.INTER_LINEAR)\n        normalized_frame = resized_frame / 255.0\n        return normalized_frame\n    except Exception as e:\n        print(f\"Error processing frame: {e}\")\n        return None\n\ndef frames_extraction(video_path, \n                      sequence_length=SEQUENCE_LENGTH, \n                      image_height=IMAGE_HEIGHT, \n                      image_width=IMAGE_WIDTH):\n    # Declare a list to store video frames\n    frames_list = []\n\n    # Check if video file exists\n    if not os.path.exists(video_path):\n        print(f\"Error: Video file not found at {video_path}\")\n        return None\n\n    # Read the video file using VideoCapture with optimized settings\n    video_reader = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)\n\n    # Check if the video was opened successfully\n    if not video_reader.isOpened():\n        print(f\"Error: Could not open video file {video_path}\")\n        video_reader.release()\n        return None\n\n    # Get the total number of frames in the video\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Ensure the video has enough frames\n    if video_frames_count < sequence_length:\n        print(f\"Warning: Video {video_path} has only {video_frames_count} frames, less than required {sequence_length}\")\n        video_reader.release()\n        return None\n\n    # Calculate the interval after which frames will be sampled\n    skip_frames_window = max(int(video_frames_count / sequence_length), 1)\n\n    # Pre-allocate frame indices to extract\n    frame_indices = [i * skip_frames_window for i in range(sequence_length)]\n\n    # Read and process frames in parallel\n    frames = []\n    for idx in frame_indices:\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        success, frame = video_reader.read()\n        if not success or frame is None:\n            print(f\"Warning: Failed to read frame at index {idx} from {video_path}\")\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frames.append(frame)\n\n    # Release the VideoCapture object early\n    video_reader.release()\n\n    # Ensure the correct number of frames is read\n    if len(frames) != sequence_length:\n        print(f\"Warning: Read {len(frames)} frames instead of {sequence_length} from {video_path}\")\n        return None\n\n    # Process frames in parallel using ThreadPoolExecutor\n    with ThreadPoolExecutor() as executor:\n        processed_frames = list(executor.map(\n            lambda f: resize_and_normalize_frame(f, image_height, image_width), \n            frames\n        ))\n\n    # Check for any failed frame processing\n    if any(f is None for f in processed_frames):\n        print(f\"Warning: Some frames failed to process in {video_path}\")\n        return None\n\n    # Convert to NumPy array\n    frames_array = np.array(processed_frames, dtype=np.float32)\n\n    return frames_array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.702868Z","iopub.execute_input":"2025-07-06T04:37:25.703243Z","iopub.status.idle":"2025-07-06T04:37:25.722164Z","shell.execute_reply.started":"2025-07-06T04:37:25.703227Z","shell.execute_reply":"2025-07-06T04:37:25.721616Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":223},{"cell_type":"code","source":" # # Iterate through all video files\n #        for file_name in files_list:\n #            video_file_path = os.path.join(class_path, file_name)\n\n #            # Extract frames using the updated frames_extraction function\n #            frames = frames_extraction(video_file_path, sequence_length, image_height, image_width)\n\n #            # Skip videos where frame extraction failed\n #            if frames is None:\n #                print(f\"Skipping video {video_file_path} due to frame extraction failure\")\n #                continue\n\n #            # Append the data to respective lists\n #            features.append(frames)\n #            labels.append(class_index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.722842Z","iopub.execute_input":"2025-07-06T04:37:25.723120Z","iopub.status.idle":"2025-07-06T04:37:25.737407Z","shell.execute_reply.started":"2025-07-06T04:37:25.723090Z","shell.execute_reply":"2025-07-06T04:37:25.736688Z"}},"outputs":[],"execution_count":224},{"cell_type":"markdown","source":"## Create Dataset","metadata":{}},{"cell_type":"code","source":"# RUN create dataset function definition\ndef create_dataset(dataset_dir,\n                   classes_list, \n                   sequence_length=SEQUENCE_LENGTH, \n                   image_height=IMAGE_HEIGHT, \n                   image_width=IMAGE_WIDTH, \n                   max_videos_per_class=None,\n                   augmentations=False\n                  ):\n    \n    # Initialize lists to store features, labels, and video file paths\n    videos = []\n    labels = []\n\n    # Check if dataset directory exists\n    if not os.path.exists(dataset_dir):\n        raise FileNotFoundError(f\"Dataset directory not found: {dataset_dir}\")\n\n    # Iterate through all classes in the classes list\n    for class_index, class_name in enumerate(classes_list):\n        class_path = os.path.join(dataset_dir, class_name)\n        \n        # Check if class directory exists\n        if not os.path.exists(class_path):\n            print(f\"Warning: Class directory not found: {class_path}\")\n            continue\n\n        print(f'Class {class_index}: {class_name}')\n\n        # Get the list of video files in the class directory\n        files_list = os.listdir(class_path)\n\n        # Limit the number of videos if specified\n        if max_videos_per_class is not None:\n            files_list = files_list[:max_videos_per_class]\n\n        # Iterate through all video files\n        is_last_video=False\n        for (index, file_name) in enumerate(files_list):\n            video_file_path = os.path.join(class_path, file_name)\n\n            if index == (len(files_list)-1):\n                is_last_video=True\n                # print(f'Extracting frames from file {index+1}: {file_name}', flush=True)\n            else:\n                is_last_video=False\n                # print(f'Extracting frames from file {index+1}: {file_name}', end=\"\\r\", flush=True)\n            \n            # Extract frames using the updated frames_extraction function\n            # frames = frames_extraction(video_file_path, \n            #                            sequence_length, \n            #                            image_height, \n            #                            image_width)\n            frames, index = scen_keyframe_extraction(video_path=video_file_path, \n                                                     file_name=file_name,\n                                                     video_index=index,\n                                                     is_last_video=is_last_video,\n                                                     num_segments=SEQUENCE_LENGTH)\n\n            # Skip videos where frame extraction failed\n            if frames is None:\n                print(f\"Skipping video {video_file_path} due to frame extraction failure\")\n                continue\n\n            # Append the data to respective lists\n            videos.append(frames)\n            labels.append(class_index)\n\n    # Convert lists to numpy arrays\n    if not videos:\n        raise ValueError(\"No valid videos were processed. Check dataset or parameters.\")\n    videos = np.asarray(videos)\n    labels = np.array(labels)\n\n    print(f\"Dataset created with {len(videos)} videos\")\n    print(f\"Features shape: {videos.shape}\")\n    print(f\"Labels shape: {labels.shape}\")\n\n    return videos, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.738139Z","iopub.execute_input":"2025-07-06T04:37:25.738328Z","iopub.status.idle":"2025-07-06T04:37:25.750537Z","shell.execute_reply.started":"2025-07-06T04:37:25.738314Z","shell.execute_reply":"2025-07-06T04:37:25.749751Z"}},"outputs":[],"execution_count":225},{"cell_type":"markdown","source":"## ü™Ñ Video Data Generator","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence, to_categorical\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\n\n\n\nclass VideoDataGenerator(Sequence):\n    def __init__(self, videos, labels, batch_size, is_training=False):\n        self.videos = list(videos)  # List of video file paths\n        self.labels = list(labels)  # List or array of labels encoded\n        self.batch_size = batch_size\n        self.indices = np.arange(len(self.videos))  # For shuffling\n        self.is_training = is_training  # Flag to control whether augmentation is applied\n        self.temp_videos=[]\n        self.temp_labels=[]\n        \n        \n        print(f\"Total {len(videos)} videos and {len(labels)} classes\")\n\n    def __len__(self):\n        # Return the number of batches per epoch\n        return int(np.ceil(len(self.videos) / self.batch_size))\n\n    def __getitem__(self, idx):\n        # Get batch indices\n        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        # Handle case where batch_indices is empty\n        if len(batch_indices) == 0:\n            return np.array([], dtype=np.float32), np.array([], dtype=np.float32)\n        \n        # Initialize lists to store frames and labels\n        all_frames = []\n        all_labels = []\n        \n        # Process each video in the batch\n        for i, index in enumerate(batch_indices):\n            sequence = self.videos[index]\n            label = self.labels[index]\n            \n            # Append the sequence and label\n            all_frames.append(sequence)\n            all_labels.append(label)\n        \n        # Convert to numpy arrays\n        all_frames = np.array(all_frames, dtype=np.float32)\n        all_labels = np.array(all_labels)\n            \n        \n        return all_frames, all_labels\n    def add(self, new_videos, new_labels):\n        self.temp_videos.extend(new_videos)\n        self.temp_labels.extend(new_labels)\n    \n    def confirm(self):\n        self.videos.extend(self.temp_videos)\n        self.labels.extend(self.temp_labels)\n        print(f\"Successfully added - {len(self.temp_videos)} videos and {len(self.temp_labels)} classes.\")\n        self.temp_videos=[]\n        self.temp_labels=[]\n        self.indices = np.arange(len(self.videos))\n        print(f\"Now Total - {len(self.videos)} videos and {len(self.labels)} classes.\")\n        print(f\"PENDING for Augmentations - {len(self.temp_videos)} videos and {len(self.temp_labels)} classes.\")\n        \n    def replace_original(self):\n        self.videos=self.temp_videos\n        self.labels=self.temp_labels\n        self.temp_videos=[]\n        self.temp_labels=[]\n        print(f\"Successfully replaced, total original videos - {len(self.videos)} and augmented videos {len(self.temp_videos)}\")\n        \n    def on_epoch_end(self):\n        # Shuffle indices at the end of each epoch\n        np.random.shuffle(self.indices)\n\n    def as_dataset(self):\n        def generator():\n            for idx in range(len(self)):\n                frames, labels = self[idx]\n                # Skip empty batches\n                if frames.size == 0:\n                    continue\n                yield frames, labels\n        self.labels=np.array(self.labels)\n    \n        # Create a tf.data.Dataset\n        dataset = tf.data.Dataset.from_generator(\n            generator,\n            output_types=(tf.float32, tf.float32),\n            output_shapes=(\n                (None, SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3),\n                (None, NO_OF_CLASSES)\n            )\n        )\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.751308Z","iopub.execute_input":"2025-07-06T04:37:25.751604Z","iopub.status.idle":"2025-07-06T04:37:25.774086Z","shell.execute_reply.started":"2025-07-06T04:37:25.751564Z","shell.execute_reply":"2025-07-06T04:37:25.773403Z"}},"outputs":[],"execution_count":226},{"cell_type":"markdown","source":"## üî• Segmentation using N-Frames Ensemble","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom collections import deque\nimport mediapipe as mp\n\n# Parameters\nN_FRAMES = 5  # Number of frames for ensemble\nTHRESHOLD = 0.2  # Segmentation confidence threshold\n\ndef initialize_segmentor():\n    \"\"\"Initialize MediaPipe Selfie Segmentation with error handling.\"\"\"\n    try:\n        mp_selfie_segmentation = mp.solutions.selfie_segmentation\n        segmentor = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)  # 1 for general model\n        print(\"MediaPipe initialized successfully\")\n        return segmentor\n    except Exception as e:\n        print(f\"Failed to initialize MediaPipe: {e}\")\n        return None\n\n# Initialize segmentor\nsegmentor = initialize_segmentor()\n\n\ndef segmentation_frames(sequence):\n    \"\"\"Process a sequence of frames with N-frame ensemble segmentation.\"\"\"\n    # Input sequence: (sequence_length, height, width, 3), values in [0, 1]\n    sequence_length, height, width, _ = sequence.shape\n    segmented_sequence = np.zeros_like(sequence, dtype=np.float32)\n    \n    if segmentor is None:\n        print(\"Returning original sequence due to initialization failure\")\n        return sequence  # Fallback to original sequence\n    # Queue to store segmentation masks\n    mask_queue = deque(maxlen=N_FRAMES)\n    \n    # Kernel for morphological operations\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    \n    for t in range(sequence_length):\n        try:\n            # Convert frame to uint8 for MediaPipe\n            frame = (sequence[t] * 255).astype(np.uint8)\n            \n            # Process frame with MediaPipe\n            results = segmentor.process(frame)\n            if results.segmentation_mask is None:\n                print(f\"Warning: No mask returned for frame {t}\")\n                mask = np.zeros((height, width), dtype=np.float32)\n            else:\n                mask = results.segmentation_mask  # Shape: (height, width), values in [0, 1]\n            \n            # Add mask to queue\n            mask_queue.append(mask)\n            \n            # Compute ensemble mask by averaging (if enough frames)\n            ensemble_mask = np.mean(np.array(mask_queue), axis=0) if len(mask_queue) > 1 else mask\n            \n            # Binarize the ensemble mask\n            binary_mask = (ensemble_mask > THRESHOLD).astype(np.uint8)\n            \n            # Refine the mask using morphological operations\n            binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n            binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)\n            \n            # Convert binary mask to 3 channels\n            mask_3ch = np.stack([binary_mask] * 3, axis=-1)  # Shape: (height, width, 3)\n            \n            # Apply mask to isolate human subject (set background to black)\n            segmented_frame = frame * mask_3ch\n            \n            # Normalize back to [0, 1]\n            segmented_sequence[t] = segmented_frame / 255.0\n        \n        except Exception as e:\n            print(f\"Error processing frame {t}: {e}\")\n            segmented_sequence[t] = sequence[t]  # Fallback to original frame\n    \n    return segmented_sequence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.776386Z","iopub.execute_input":"2025-07-06T04:37:25.776628Z","iopub.status.idle":"2025-07-06T04:37:25.794896Z","shell.execute_reply.started":"2025-07-06T04:37:25.776613Z","shell.execute_reply":"2025-07-06T04:37:25.794195Z"}},"outputs":[{"name":"stdout","text":"MediaPipe initialized successfully\n","output_type":"stream"}],"execution_count":227},{"cell_type":"markdown","source":"## üí´ Augmentation Helper function","metadata":{}},{"cell_type":"code","source":"def apply_augmentation(augmentation_function, generator, confirm=False, *args, **kwargs):\n    count=0\n    for i in range(len(generator)):\n        batch_videos, batch_labels = generator[i]\n        augmented_videos=[]\n        for videos in batch_videos:\n            count=count+1\n            augmented_videos.append(augmentation_function(videos, *args, **kwargs))\n            print(f\"Adding {count} new videos...\", end=\"\\r\", flush=True)\n        generator.add(augmented_videos, batch_labels)\n    \n    print(f\"Total {len(generator.temp_videos)} augmented videos generated\")\n    if confirm:\n        generator.confirm()\n    else:\n        print(\"Ready to be merged with original videos...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.795926Z","iopub.execute_input":"2025-07-06T04:37:25.796199Z","iopub.status.idle":"2025-07-06T04:37:25.812604Z","shell.execute_reply.started":"2025-07-06T04:37:25.796165Z","shell.execute_reply":"2025-07-06T04:37:25.812003Z"}},"outputs":[{"name":"stderr","text":"W0000 00:00:1751776645.800020   90467 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"}],"execution_count":228},{"cell_type":"markdown","source":"### Show Frames","metadata":{}},{"cell_type":"code","source":"# show frame def\nimport matplotlib.pyplot as plt\n\ndef show_frame(frame):\n    plt.figure(figsize=(3,3))\n    plt.imshow(frame)\n    plt.axis(\"off\")\n    plt.tight_layout()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.813444Z","iopub.execute_input":"2025-07-06T04:37:25.813731Z","iopub.status.idle":"2025-07-06T04:37:25.825741Z","shell.execute_reply.started":"2025-07-06T04:37:25.813711Z","shell.execute_reply":"2025-07-06T04:37:25.825104Z"}},"outputs":[],"execution_count":229},{"cell_type":"markdown","source":"### Helper Function","metadata":{}},{"cell_type":"code","source":"def show_frame_vs(frame1, frame2, title1=\"Frame 1\", title2=\"Frame 2\"):\n    plt.figure(figsize=(10, 5))\n\n    # First frame\n    plt.subplot(1, 2, 1)\n    plt.imshow(frame1)\n    plt.title(title1)\n    plt.axis('off')\n\n    # Second frame\n    plt.subplot(1, 2, 2)\n    plt.imshow(frame2)\n    plt.title(title2)\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.826491Z","iopub.execute_input":"2025-07-06T04:37:25.826763Z","iopub.status.idle":"2025-07-06T04:37:25.839288Z","shell.execute_reply.started":"2025-07-06T04:37:25.826743Z","shell.execute_reply":"2025-07-06T04:37:25.838630Z"}},"outputs":[],"execution_count":230},{"cell_type":"code","source":"# reusable video loop helper function\ndef apply_function(function, videos, *arg, **kwargs):\n    new_videos=[]\n    \n    for video in videos:\n        new_videos.append(function(video, *arg, **kwargs))\n        \n    return new_videos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.839902Z","iopub.execute_input":"2025-07-06T04:37:25.840068Z","iopub.status.idle":"2025-07-06T04:37:25.855015Z","shell.execute_reply.started":"2025-07-06T04:37:25.840056Z","shell.execute_reply":"2025-07-06T04:37:25.854440Z"}},"outputs":[],"execution_count":231},{"cell_type":"code","source":"# reusable video loop helper function\ndef apply(function, videos, labels, *arg, **kwargs):\n    new_videos=[]\n    new_labels=[]\n    combined=zip(labels, videos)\n    \n    for label, video in combined:\n        new_videos.append(function(video, *arg, **kwargs))\n        new_labels.append(label)\n        \n    new_videos=np.asarray(new_videos)\n    new_labels=np.asarray(new_labels)\n    \n    return new_videos, new_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.856224Z","iopub.execute_input":"2025-07-06T04:37:25.856446Z","iopub.status.idle":"2025-07-06T04:37:25.868136Z","shell.execute_reply.started":"2025-07-06T04:37:25.856424Z","shell.execute_reply":"2025-07-06T04:37:25.867461Z"}},"outputs":[],"execution_count":232},{"cell_type":"markdown","source":"## Salt and Pepper Noise ","metadata":{}},{"cell_type":"code","source":"# Noise function def\n\ndef noise_video(video,noise_factor=NOISE_FACTOR):\n    noisy_video=[]\n    for frame in video:\n        # Generate random noise\n        noise = np.random.rand(*frame.shape)  # Generate random noise\n        salt_pepper_noise = np.random.choice([0, 1], size=frame.shape, p=[1-noise_factor, noise_factor])\n        noisy_frame = frame * (1 - salt_pepper_noise) + salt_pepper_noise * np.random.rand(*frame.shape)\n        noisy_video.append(noisy_frame)\n    return np.array(noisy_video)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.868834Z","iopub.execute_input":"2025-07-06T04:37:25.869057Z","iopub.status.idle":"2025-07-06T04:37:25.881258Z","shell.execute_reply.started":"2025-07-06T04:37:25.869033Z","shell.execute_reply":"2025-07-06T04:37:25.880562Z"}},"outputs":[],"execution_count":233},{"cell_type":"markdown","source":"## Horizontal Flip","metadata":{}},{"cell_type":"code","source":"# horizontal flip function def\n# Testing Flip feautes[0] -> first video\ndef horizontal_flip(video):\n    return [tf.image.flip_left_right(frame) for frame in video]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.881890Z","iopub.execute_input":"2025-07-06T04:37:25.882083Z","iopub.status.idle":"2025-07-06T04:37:25.894928Z","shell.execute_reply.started":"2025-07-06T04:37:25.882069Z","shell.execute_reply":"2025-07-06T04:37:25.894259Z"}},"outputs":[],"execution_count":234},{"cell_type":"markdown","source":"## Random Shear","metadata":{}},{"cell_type":"code","source":"# random shear function def\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.ndimage import affine_transform\n\ndef apply_shear_frames(frames, shear_right):\n    return [apply_shear(frame, shear_right) for frame in frames]\n\ndef apply_shear(frame, shear_right=0):\n    frame_resized = tf.image.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH)).numpy()\n    if shear_right:\n        shear_x = 0.09\n        shear_y = 0.1\n    else:\n        shear_x = -0.09\n        shear_y = 0.3\n    # Construct the affine matrix for scipy: inverse of the transformation matrix\n    # scipy applies the inverse transform matrix\n    shear_matrix = np.array([\n        [1, shear_x, 0],\n        [shear_y, 1, 0],\n        [0, 0, 1]\n    ], dtype=np.float32)\n\n    # Extract 2x2 part for affine_transform\n    matrix = shear_matrix[:2, :2]\n\n    # Offset (no translation)\n    offset = [0, 0]\n\n    # Apply affine transform on each channel separately\n    sheared = np.zeros_like(frame_resized)\n    for c in range(3):\n        sheared[..., c] = affine_transform(\n            frame_resized[..., c],\n            matrix=matrix,\n            offset=offset,\n            order=1,          # bilinear interpolation\n            mode='nearest'    # fill_mode\n        )\n    return sheared","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.895607Z","iopub.execute_input":"2025-07-06T04:37:25.895823Z","iopub.status.idle":"2025-07-06T04:37:25.909819Z","shell.execute_reply.started":"2025-07-06T04:37:25.895808Z","shell.execute_reply":"2025-07-06T04:37:25.909129Z"}},"outputs":[],"execution_count":235},{"cell_type":"markdown","source":"# üìå Model Related\n## üóº Model Architecture `create_model`","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import Xception\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, TimeDistributed, LSTM, Dropout, Dense, BatchNormalization\nfrom tensorflow.keras.regularizers import l2\n\ndef create_xception_lstm_model(sequence_length, image_height, image_width, classes_list=None, dropout_rate=0.5):\n    if classes_list is None:\n        raise ValueError(\"classes_list must be provided to define the output layer size\")\n\n    try:\n        # Load Xception model with pre-trained ImageNet weights\n        print(\"Loading Xception base model...\")\n        xception = Xception(\n            weights='imagenet',\n            include_top=False,\n            input_shape=(image_height, image_width, 3),\n            name=\"Xception\"\n        )\n        \n        # # Unfreeze more layers to fine-tune\n        # for layer in xception.layers[:-10]:  # Unfreeze more layers to fine-tune more\n        #     layer.trainable = False\n        # for layer in xception.layers[-10:]:\n        #     layer.trainable = True\n\n        # Define the Sequential model\n        model = Sequential([\n            # TimeDistributed Xception model\n            TimeDistributed(\n                xception,\n                input_shape=(sequence_length, image_height, image_width, 3),\n                name=\"time_distributed\"\n            ),\n            # TimeDistributed GlobalAveragePooling2D\n            TimeDistributed(GlobalAveragePooling2D(), name=\"time_distributed_1\"),\n            # LSTM layer with 256 units (increased)\n            LSTM(512, activation=\"tanh\", return_sequences=False, kernel_regularizer=l2(0.01), name=\"lstm\"),\n            BatchNormalization(name=\"batch_normalization\"),\n            # Dropout after LSTM\n            Dropout(dropout_rate, name=\"dropout_lstm\"),\n            # Dense layer with 512 units\n            Dense(512, activation=\"relu\", kernel_regularizer=l2(0.01), name=\"dense\"),\n            # Dropout after Dense\n            Dropout(dropout_rate + 0.1, name=\"dropout_dense\"),\n            # Output Dense layer with softmax activation\n            Dense(len(classes_list), activation=\"softmax\", name=\"dense_1\")\n        ])\n\n        # Print model summary\n        print(\"Model architecture created successfully!\")\n        model.summary()\n\n        return model\n\n    except Exception as e:\n        print(f\"Error creating model: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.910432Z","iopub.execute_input":"2025-07-06T04:37:25.910672Z","iopub.status.idle":"2025-07-06T04:37:25.926397Z","shell.execute_reply.started":"2025-07-06T04:37:25.910652Z","shell.execute_reply":"2025-07-06T04:37:25.925793Z"}},"outputs":[],"execution_count":236},{"cell_type":"code","source":"# donwload model weights\nfrom tensorflow.keras.applications import Xception\nprint(\"Pre-loading Xception weights...\")\nbase_model = Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\nprint(\"Weights loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:25.927024Z","iopub.execute_input":"2025-07-06T04:37:25.927225Z","iopub.status.idle":"2025-07-06T04:37:27.307275Z","shell.execute_reply.started":"2025-07-06T04:37:25.927210Z","shell.execute_reply":"2025-07-06T04:37:27.306505Z"}},"outputs":[{"name":"stdout","text":"Pre-loading Xception weights...\nWeights loaded successfully!\n","output_type":"stream"}],"execution_count":237},{"cell_type":"markdown","source":"# üìå Execution\n## Creating Dataset\nThe following functions are executed\n- `frame extraction`\n- `resize`\n\nwhich returns:\n- `features` - `np.array` of all the videos\n- `labels` - `np.array` of all the class labels","metadata":{}},{"cell_type":"code","source":"# RUN Create the dataset with explicit parameters\ntry:\n    videos, labels = create_dataset(\n        dataset_dir=DATASET_DIR,\n        classes_list=CLASSES_LIST,\n        sequence_length=SEQUENCE_LENGTH,\n        image_height=IMAGE_HEIGHT,\n        image_width=IMAGE_WIDTH,\n        # Limit to 10 videos per class to manage memory\n        augmentations=True,\n        max_videos_per_class=MAX_VIDEO_PER_CLASS\n    )\nexcept FileNotFoundError as e:\n    print(f\"Error: {e}\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n\nvideos.shape, labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T04:37:30.273106Z","iopub.execute_input":"2025-07-06T04:37:30.273373Z","iopub.status.idle":"2025-07-06T04:40:27.262489Z","shell.execute_reply.started":"2025-07-06T04:37:30.273353Z","shell.execute_reply":"2025-07-06T04:40:27.261901Z"}},"outputs":[{"name":"stdout","text":"Class 0: write on a paper\nExtracted 5 frames from file 20: a05_s10_e01_rgb.avi\nClass 1: use laptop\nExtracted 5 frames from file 20: a06_s04_e02_rgb.avi\nClass 2: read book\nExtracted 5 frames from file 20: a03_s06_e01_rgb.avi\nClass 3: sit still\nExtracted 5 frames from file 20: a09_s06_e02_rgb.avi\nClass 4: drink\nExtracted 5 frames from file 20: a01_s01_e01_rgb.avi\nClass 5: sit down\nExtracted 5 frames from file 20: a16_s06_e02_rgb.avi\nClass 6: use vacuum cleaner\nExtracted 5 frames from file 20: a07_s10_e01_rgb.avi\nClass 7: eat\nExtracted 5 frames from file 20: a02_s02_e01_rgb.avi\nClass 8: play guitar\nExtracted 5 frames from file 20: a14_s06_e02_rgb.avi\nClass 9: lie down on sofa\nExtracted 5 frames from file 20: a12_s05_e01_rgb.avi\nClass 10: stand up\nExtracted 5 frames from file 20: a15_s09_e01_rgb.avi\nClass 11: toss paper\nExtracted 5 frames from file 20: a10_s04_e01_rgb.avi\nClass 12: walk\nExtracted 5 frames from file 20: a13_s09_e01_rgb.avi\nClass 13: play game\nExtracted 5 frames from file 20: a11_s01_e02_rgb.avi\nClass 14: cheer up\nExtracted 5 frames from file 20: a08_s07_e02_rgb.avi\nClass 15: call cellphone\nExtracted 5 frames from file 20: a04_s06_e01_rgb.avi\nDataset created with 320 videos\nFeatures shape: (320, 5, 128, 128, 3)\nLabels shape: (320,)\n","output_type":"stream"},{"execution_count":238,"output_type":"execute_result","data":{"text/plain":"((320, 5, 128, 128, 3), (320,))"},"metadata":{}}],"execution_count":238},{"cell_type":"code","source":"def show_sequence(frames, index):\n    num_keyframes = len(frames)\n    plt.figure(figsize=(5 * num_keyframes, 5))\n\n    for plot_index, (i, frame) in enumerate(zip(index, frames), start=1):\n        frame_label = f\"(Frame {i})\"\n        frame = frame.copy()  # Ensure cv2.putText doesn't affect original\n        cv2.putText(frame, frame_label, (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n        plt.subplot(1, num_keyframes, plot_index)\n        plt.imshow(frame)\n        plt.axis('off')\n\n    plt.tight_layout()\n    plt.savefig('ucf50_biking_keyframe_summary.jpg', bbox_inches='tight', dpi=300)\n    plt.show()\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.144740Z","iopub.status.idle":"2025-07-06T03:41:24.144970Z","shell.execute_reply.started":"2025-07-06T03:41:24.144863Z","shell.execute_reply":"2025-07-06T03:41:24.144874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check summarization\nshow_sequence(videos[0], [1,2,3,4,5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.145884Z","iopub.status.idle":"2025-07-06T03:41:24.146228Z","shell.execute_reply.started":"2025-07-06T03:41:24.146049Z","shell.execute_reply":"2025-07-06T03:41:24.146066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_generator = VideoDataGenerator(\n    videos=videos,\n    labels=labels,\n    batch_size=BATCH_SIZE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.147115Z","iopub.status.idle":"2025-07-06T03:41:24.147394Z","shell.execute_reply.started":"2025-07-06T03:41:24.147225Z","shell.execute_reply":"2025-07-06T03:41:24.147235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"videos, labels = video_generator[0]\n\nvideos.shape, labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.148700Z","iopub.status.idle":"2025-07-06T03:41:24.148921Z","shell.execute_reply.started":"2025-07-06T03:41:24.148814Z","shell.execute_reply":"2025-07-06T03:41:24.148829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_generator.videos) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.150196Z","iopub.status.idle":"2025-07-06T03:41:24.150459Z","shell.execute_reply.started":"2025-07-06T03:41:24.150311Z","shell.execute_reply":"2025-07-06T03:41:24.150326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.151283Z","iopub.status.idle":"2025-07-06T03:41:24.151555Z","shell.execute_reply.started":"2025-07-06T03:41:24.151437Z","shell.execute_reply":"2025-07-06T03:41:24.151450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"apply_augmentation(segmentation_frames, video_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.152249Z","iopub.status.idle":"2025-07-06T03:41:24.152483Z","shell.execute_reply.started":"2025-07-06T03:41:24.152371Z","shell.execute_reply":"2025-07-06T03:41:24.152385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_generator.temp_videos), len(video_generator.videos)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.153566Z","iopub.status.idle":"2025-07-06T03:41:24.153854Z","shell.execute_reply.started":"2025-07-06T03:41:24.153727Z","shell.execute_reply":"2025-07-06T03:41:24.153740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_generator.replace_original()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.154704Z","iopub.status.idle":"2025-07-06T03:41:24.155036Z","shell.execute_reply.started":"2025-07-06T03:41:24.154874Z","shell.execute_reply":"2025-07-06T03:41:24.154890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_generator.temp_videos), len(video_generator.videos)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.156217Z","iopub.status.idle":"2025-07-06T03:41:24.156501Z","shell.execute_reply.started":"2025-07-06T03:41:24.156375Z","shell.execute_reply":"2025-07-06T03:41:24.156388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = np.array(video_generator.videos)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.157531Z","iopub.status.idle":"2025-07-06T03:41:24.157795Z","shell.execute_reply.started":"2025-07-06T03:41:24.157691Z","shell.execute_reply":"2025-07-06T03:41:24.157701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_frame(test[1,0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.158849Z","iopub.status.idle":"2025-07-06T03:41:24.159121Z","shell.execute_reply.started":"2025-07-06T03:41:24.159002Z","shell.execute_reply":"2025-07-06T03:41:24.159015Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply ***horizontal flip*** (Augmentation)","metadata":{}},{"cell_type":"code","source":"apply_augmentation(horizontal_flip, video_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.160016Z","iopub.status.idle":"2025-07-06T03:41:24.160220Z","shell.execute_reply.started":"2025-07-06T03:41:24.160124Z","shell.execute_reply":"2025-07-06T03:41:24.160133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Checking Total Videos and Class Count: {len(video_generator.videos)}, {len(video_generator.labels)}\")\nprint(f\"PENDING Total Videos and Class Count: {len(video_generator.temp_videos)}, {len(video_generator.temp_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.161080Z","iopub.status.idle":"2025-07-06T03:41:24.161345Z","shell.execute_reply.started":"2025-07-06T03:41:24.161202Z","shell.execute_reply":"2025-07-06T03:41:24.161216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_generator.confirm()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.162774Z","iopub.status.idle":"2025-07-06T03:41:24.163085Z","shell.execute_reply.started":"2025-07-06T03:41:24.162928Z","shell.execute_reply":"2025-07-06T03:41:24.162943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply ***Noise*** (Augmentation)","metadata":{}},{"cell_type":"code","source":"apply_augmentation(noise_video, video_generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.164253Z","iopub.status.idle":"2025-07-06T03:41:24.164548Z","shell.execute_reply.started":"2025-07-06T03:41:24.164388Z","shell.execute_reply":"2025-07-06T03:41:24.164403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Checking Total Videos and Class Count: {len(video_generator.videos)}, {len(video_generator.labels)}\")\nprint(f\"PENDING Total Videos and Class Count: {len(video_generator.temp_videos)}, {len(video_generator.temp_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.165764Z","iopub.status.idle":"2025-07-06T03:41:24.165991Z","shell.execute_reply.started":"2025-07-06T03:41:24.165891Z","shell.execute_reply":"2025-07-06T03:41:24.165900Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply ***Random Shear x2*** (Augmentation)","metadata":{}},{"cell_type":"code","source":"apply_augmentation(apply_shear_frames, video_generator, shear_right=0)\napply_augmentation(apply_shear_frames, video_generator, shear_right=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.166787Z","iopub.status.idle":"2025-07-06T03:41:24.167092Z","shell.execute_reply.started":"2025-07-06T03:41:24.166944Z","shell.execute_reply":"2025-07-06T03:41:24.166959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Checking Total Videos and Class Count: {len(video_generator.videos)}, {len(video_generator.labels)}\")\nprint(f\"PENDING Total Videos and Class Count: {len(video_generator.temp_videos)}, {len(video_generator.temp_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.168848Z","iopub.status.idle":"2025-07-06T03:41:24.169108Z","shell.execute_reply.started":"2025-07-06T03:41:24.168969Z","shell.execute_reply":"2025-07-06T03:41:24.168983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# confirm changes to generator\nvideo_generator.confirm()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.169886Z","iopub.status.idle":"2025-07-06T03:41:24.170189Z","shell.execute_reply.started":"2025-07-06T03:41:24.170034Z","shell.execute_reply":"2025-07-06T03:41:24.170048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Checking Video Generator Total Videos and Class Count: {len(video_generator.videos)}, {len(video_generator.labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.171234Z","iopub.status.idle":"2025-07-06T03:41:24.171440Z","shell.execute_reply.started":"2025-07-06T03:41:24.171344Z","shell.execute_reply":"2025-07-06T03:41:24.171352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cheking (Output)","metadata":{}},{"cell_type":"code","source":"# check horizontal flips\nvideos, labels = video_generator[7]\nvideos1, labels1 = video_generator[1]\n\nprint(videos.shape, labels.shape)\n\nshow_frame_vs(videos[0,0],videos1[1,0], title1=\"Flipped Frame 1\", title2=\"Non Flipped Frame 1\")\nprint(\"Might be two different action, but to check the flipping focus on the door behind.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.172709Z","iopub.status.idle":"2025-07-06T03:41:24.172952Z","shell.execute_reply.started":"2025-07-06T03:41:24.172830Z","shell.execute_reply":"2025-07-06T03:41:24.172842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß™ Test Augmentation (Output)\n## üñºÔ∏è Horizontal Flip Output","metadata":{}},{"cell_type":"code","source":"# flipped_video_1 = horizontal_flip(features[0])\n\n# flipped_video_1=np.asarray(flipped_video_1)\n\n# show_frame(flipped_video_1[0])\n# flipped_video_1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.173620Z","iopub.status.idle":"2025-07-06T03:41:24.173843Z","shell.execute_reply.started":"2025-07-06T03:41:24.173744Z","shell.execute_reply":"2025-07-06T03:41:24.173752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üñºÔ∏è Salt and Pepper Noise Output","metadata":{}},{"cell_type":"code","source":"# noised_video = noise_video(features[0])\n\n# print(noised_video.shape)\n# show_frame(noised_video[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.175078Z","iopub.status.idle":"2025-07-06T03:41:24.175400Z","shell.execute_reply.started":"2025-07-06T03:41:24.175235Z","shell.execute_reply":"2025-07-06T03:41:24.175249Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üñºÔ∏è Random Shear Output","metadata":{}},{"cell_type":"code","source":"# print(features.shape)\n\n# sheared_1 = apply_shear_frames(features[0], shear_right=0)\n# sheared_2 = apply_shear_frames(features[0], shear_right=1)\n\n# sheared_1 = np.asarray(sheared_1)\n# sheared_2 = np.asarray(sheared_2)\n\n# print(sheared_1.shape)\n# show_frame(sheared_1[0])\n# show_frame(sheared_2[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.176472Z","iopub.status.idle":"2025-07-06T03:41:24.176791Z","shell.execute_reply.started":"2025-07-06T03:41:24.176637Z","shell.execute_reply":"2025-07-06T03:41:24.176652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìå Preparing Data for Training","metadata":{}},{"cell_type":"code","source":"# Clear previous session to free memory\nimport gc\n\ngc.collect()\ntf.keras.backend.clear_session()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.177782Z","iopub.status.idle":"2025-07-06T03:41:24.178091Z","shell.execute_reply.started":"2025-07-06T03:41:24.177936Z","shell.execute_reply":"2025-07-06T03:41:24.177950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting data for ***Trainig*** and ***Testing***","metadata":{}},{"cell_type":"code","source":"videos = video_generator.videos\nlabels = video_generator.labels\n\nvideos = np.array(videos, dtype='float32')\nlabels = np.array(labels)\n\nvideos.shape, labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.179458Z","iopub.status.idle":"2025-07-06T03:41:24.179789Z","shell.execute_reply.started":"2025-07-06T03:41:24.179625Z","shell.execute_reply":"2025-07-06T03:41:24.179639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\none_hot_encoded_labels = to_categorical(labels)\n\none_hot_encoded_labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.180704Z","iopub.status.idle":"2025-07-06T03:41:24.181010Z","shell.execute_reply.started":"2025-07-06T03:41:24.180855Z","shell.execute_reply":"2025-07-06T03:41:24.180868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.181874Z","iopub.status.idle":"2025-07-06T03:41:24.182097Z","shell.execute_reply.started":"2025-07-06T03:41:24.181991Z","shell.execute_reply":"2025-07-06T03:41:24.182002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# RUN Split the Data into Train ( 75% ) and Test Set ( 25% ).\nfrom sklearn.model_selection import train_test_split\nfeatures_train, features_test, labels_train, labels_test = train_test_split(videos,\n                                                                            one_hot_encoded_labels,\n                                                                            test_size = TEST_SIZE,\n                                                                            shuffle = True,\n                                                                            random_state = seed_constant)\n\nfeatures_train.shape, labels_train.shape, features_test.shape, labels_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.182962Z","iopub.status.idle":"2025-07-06T03:41:24.183277Z","shell.execute_reply.started":"2025-07-06T03:41:24.183120Z","shell.execute_reply":"2025-07-06T03:41:24.183134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting data for ***Training Set*** and ***Validation Set***","metadata":{}},{"cell_type":"code","source":"# Assuming features_train and labels_train are defined\ntrain_set = 1-VALIDATION_SPLIT\n\ntrain_video_frames, val_video_frames = features_train[:int(train_set * len(features_train))], features_train[int(train_set * len(features_train)):]\ntrain_labels, val_labels = labels_train[:int(train_set * len(labels_train))], labels_train[int(train_set * len(labels_train)):]\n\ntrain_video_frames.shape, train_labels.shape, val_video_frames.shape, val_labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.184380Z","iopub.status.idle":"2025-07-06T03:41:24.184726Z","shell.execute_reply.started":"2025-07-06T03:41:24.184565Z","shell.execute_reply":"2025-07-06T03:41:24.184597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Use Generator `Video Data Generator`","metadata":{}},{"cell_type":"code","source":"# Create the training and validation generators\ntrain_gen = VideoDataGenerator(\n    videos=train_video_frames, \n    labels=train_labels, \n    batch_size=BATCH_SIZE,\n)\n\nval_gen = VideoDataGenerator(\n    videos=val_video_frames, \n    labels=val_labels, \n    batch_size=BATCH_SIZE,\n)\n\nlen(train_gen), len(val_gen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.186072Z","iopub.status.idle":"2025-07-06T03:41:24.186340Z","shell.execute_reply.started":"2025-07-06T03:41:24.186214Z","shell.execute_reply":"2025-07-06T03:41:24.186226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inspect ***Generator*** Data on Epochs","metadata":{}},{"cell_type":"code","source":"features, labels = train_gen[1]\n\nprint(train_gen.indices)\n\ntrain_gen.on_epoch_end()\n\nprint(train_gen.indices)\n\nBATCH_SIZE, features.shape, labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.187706Z","iopub.status.idle":"2025-07-06T03:41:24.187989Z","shell.execute_reply.started":"2025-07-06T03:41:24.187823Z","shell.execute_reply":"2025-07-06T03:41:24.187835Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compute Class Weight","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# Assuming y_int are integer labels (not one-hot)\ny_int = np.argmax(one_hot_encoded_labels, axis=1)  # or just integer labels if you have them\n\nclasses = np.arange(NO_OF_CLASSES)  # e.g. np.arange(16)\nclass_weights_values = compute_class_weight(class_weight='balanced', classes=classes, y=y_int)\n\nclass_weights = dict(zip(classes, class_weights_values))\n\nprint(\"Class weights:\", class_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.188844Z","iopub.status.idle":"2025-07-06T03:41:24.189158Z","shell.execute_reply.started":"2025-07-06T03:41:24.189034Z","shell.execute_reply":"2025-07-06T03:41:24.189048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\ndel videos, one_hot_encoded_labels, labels_train\ndel features_train, val_labels, train_labels\ndel val_video_frames, train_video_frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.190115Z","iopub.status.idle":"2025-07-06T03:41:24.190353Z","shell.execute_reply.started":"2025-07-06T03:41:24.190253Z","shell.execute_reply":"2025-07-06T03:41:24.190262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ‚ú® Model Execution\n## Creating Model","metadata":{}},{"cell_type":"code","source":"# Create the model\nxlstm_model = create_xception_lstm_model(\n    sequence_length=SEQUENCE_LENGTH,\n    image_height=IMAGE_HEIGHT,\n    image_width=IMAGE_WIDTH,\n    classes_list=CLASSES_LIST,\n    dropout_rate=DROPOUT_RATE\n)\n\n# Check if model was created successfully\nif xlstm_model is None:\n    print(\"Failed to create model. Check error messages above.\")\nelse:\n    print(\"Model Created Successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.191893Z","iopub.status.idle":"2025-07-06T03:41:24.192197Z","shell.execute_reply.started":"2025-07-06T03:41:24.192041Z","shell.execute_reply":"2025-07-06T03:41:24.192054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot Model Structure","metadata":{}},{"cell_type":"code","source":"# Plot the structure of the contructed model.\nfrom tensorflow.keras.utils import plot_model\n\nplot_model(xlstm_model, to_file = f'{MODEL_NAME}_model_Plot.png', show_shapes = True, show_layer_names = True)\n\nprint(f\"{MODEL_NAME} Model Plot saved successfully...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.193243Z","iopub.status.idle":"2025-07-06T03:41:24.193703Z","shell.execute_reply.started":"2025-07-06T03:41:24.193507Z","shell.execute_reply":"2025-07-06T03:41:24.193526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Compile","metadata":{}},{"cell_type":"code","source":"# Create an Instance of Early Stopping Callback\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping_callback = EarlyStopping(monitor = 'val_loss', \n                                        patience = 7, \n                                        mode = 'min', \n                                        restore_best_weights = True)\n\n# Compile the model and specify loss function, optimizer and metrics values to the model\nxlstm_model.compile(loss = 'categorical_crossentropy', \n                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                    metrics = [\"accuracy\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.195002Z","iopub.status.idle":"2025-07-06T03:41:24.195290Z","shell.execute_reply.started":"2025-07-06T03:41:24.195119Z","shell.execute_reply":"2025-07-06T03:41:24.195133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üí™ Model Fit","metadata":{}},{"cell_type":"code","source":"labels = np.argmax(train_gen.labels, axis=1)  # assuming one-hot encoded\nprint(\"Unique labels:\", np.unique(labels))\nprint(\"NO_OF_CLASSES:\", NO_OF_CLASSES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.196188Z","iopub.status.idle":"2025-07-06T03:41:24.196459Z","shell.execute_reply.started":"2025-07-06T03:41:24.196301Z","shell.execute_reply":"2025-07-06T03:41:24.196315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train label shape:\", np.shape(train_gen.labels))\nprint(\"Val label shape:\", np.shape(val_gen.labels))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.198103Z","iopub.status.idle":"2025-07-06T03:41:24.198389Z","shell.execute_reply.started":"2025-07-06T03:41:24.198222Z","shell.execute_reply":"2025-07-06T03:41:24.198234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nlr_scheduler = ReduceLROnPlateau(monitor='val_loss', \n                                 factor=0.5, \n                                 patience=3, \n                                 min_lr=1e-6)\n\n\n\nprint(f\"Dataset - {DATASET_NAME}\")\nprint(f\"There are total {len(ALL_CLASS_NAMES)} classes, selected {NO_OF_CLASSES} classes\")\nprint(f\"Setting {MAX_VIDEO_PER_CLASS} videos/class to train the model.\")\nprint(f\"Image size {IMAGE_HEIGHT}x{IMAGE_WIDTH} with {SEQUENCE_LENGTH} sequence length\")\nprint(f\"Dropout rate: {DROPOUT_RATE}\")\nprint(f\"Train-Test split ratio {int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}\")\nprint(f\"Validation data from Train set {VALIDATION_SPLIT*100}%\")\n\nprint(\"\\n\")\n\nprint(f\"Training started for {len(train_gen.videos)} videos of {NO_OF_CLASSES} classes with {len(val_gen.videos)} videos for validation...\")\n\nmodel_history = xlstm_model.fit(\n    train_gen.as_dataset(),\n    validation_data=val_gen.as_dataset(),\n    epochs=EPOCHS,\n    # class_weight=class_weight_dict,\n    callbacks=[early_stopping_callback, lr_scheduler]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.199248Z","iopub.status.idle":"2025-07-06T03:41:24.199554Z","shell.execute_reply.started":"2025-07-06T03:41:24.199399Z","shell.execute_reply":"2025-07-06T03:41:24.199412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## ‚ö° Model Evaluate","metadata":{}},{"cell_type":"code","source":"# previous code\nmodel_evaluation_history = xlstm_model.evaluate(features_test, labels_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.201009Z","iopub.status.idle":"2025-07-06T03:41:24.201298Z","shell.execute_reply.started":"2025-07-06T03:41:24.201123Z","shell.execute_reply":"2025-07-06T03:41:24.201135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(model_history.history['accuracy'], label='Train Accuracy')\nplt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss\nplt.subplot(1, 2, 2)\nplt.plot(model_history.history['loss'], label='Train Loss')\nplt.plot(model_history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.202464Z","iopub.status.idle":"2025-07-06T03:41:24.202744Z","shell.execute_reply.started":"2025-07-06T03:41:24.202597Z","shell.execute_reply":"2025-07-06T03:41:24.202612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìà Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create test generator with batch_size matching test set size\ntest_gen = VideoDataGenerator(\n    videos=features_test,\n    labels=labels_test,\n    batch_size=4,  # Process all test samples at once\n)\n\n# Get predictions\npredictions = xlstm_model.predict(test_gen.as_dataset())\npredicted_classes = np.argmax(predictions, axis=1)\ntrue_classes = np.argmax(labels_test, axis=1)\n\n# Verify shapes\nprint(f\"True classes shape: {true_classes.shape}\")\nprint(f\"Predicted classes shape: {predicted_classes.shape}\")\nassert len(true_classes) == len(predicted_classes), \"Sample counts do not match!\"\n\n# Generate classification report\nprint(\"Classification Report:\")\nprint(classification_report(true_classes, predicted_classes, target_names=CLASSES_LIST))\n\n# Plot confusion matrix\ncm = confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES_LIST, yticklabels=CLASSES_LIST[-1])\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T03:41:24.203721Z","iopub.status.idle":"2025-07-06T03:41:24.204039Z","shell.execute_reply.started":"2025-07-06T03:41:24.203884Z","shell.execute_reply":"2025-07-06T03:41:24.203898Z"}},"outputs":[],"execution_count":null}]}